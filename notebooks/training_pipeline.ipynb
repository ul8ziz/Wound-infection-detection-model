{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Wound Infection Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Part 1: Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âœ“ All libraries imported successfully!\n",
      "============================================================\n",
      "PyTorch: 2.5.1+cu121\n",
      "NumPy: 2.3.5\n",
      "CUDA: True\n",
      "Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Import all required libraries\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    import json\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from pathlib import Path\n",
    "    from typing import Dict, List, Tuple\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    from tqdm import tqdm\n",
    "    import random\n",
    "    import yaml\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # PyTorch Vision\n",
    "    import torchvision\n",
    "    from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "    from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ“ All libraries imported successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"NumPy: {np.__version__}\")\n",
    "    print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except ValueError as e:\n",
    "    if \"numpy.dtype size changed\" in str(e):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"âŒ Ø®Ø·Ø£: ØªØ¹Ø§Ø±Ø¶ Ø¨ÙŠÙ† numpy Ùˆ scipy\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nðŸ”§ Ø§Ù„Ø­Ù„:\")\n",
    "        print(\"   1. Ø´ØºÙ‘Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙˆØ§Ù…Ø± ÙÙŠ Terminal:\")\n",
    "        print(\"      pip install --upgrade --force-reinstall numpy scipy\")\n",
    "        print(\"   2. Ø£Ùˆ Ø´ØºÙ‘Ù„ Part 0 Ù…Ø±Ø© Ø£Ø®Ø±Ù‰\")\n",
    "        print(\"   3. Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Kernel: Kernel â†’ Restart\")\n",
    "        print(\"=\" * 60)\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Ø§Ù„Ø®Ø·Ø£: {e}\")\n",
    "    print(\"\\nðŸ”§ Ø§Ù„Ø­Ù„:\")\n",
    "    print(\"   1. Ø´ØºÙ‘Ù„ Part 0 Ø£ÙˆÙ„Ø§Ù‹ (ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª)\")\n",
    "    print(\"   2. Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Kernel: Kernel â†’ Restart\")\n",
    "    print(\"=\" * 60)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Part 2: Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“¦ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\n",
      "============================================================\n",
      "\n",
      "[1/3] ØªØ«Ø¨ÙŠØª setuptools Ùˆ wheel...\n",
      "  ðŸ“¦ setuptools... âœ“\n",
      "  ðŸ“¦ wheel... âœ“\n",
      "\n",
      "[2/3] ØªØ«Ø¨ÙŠØª numpy Ùˆ scipy...\n",
      "  ðŸ“¦ numpy>=1.26.0... âœ“\n",
      "  ðŸ“¦ scipy>=1.11.0... âœ“\n",
      "\n",
      "[3/3] ØªØ«Ø¨ÙŠØª Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\n",
      "  ðŸ“¦ torch... âœ“\n",
      "  ðŸ“¦ torchvision... âœ“\n",
      "  ðŸ“¦ opencv-python... âœ“\n",
      "  ðŸ“¦ Pillow... âœ“\n",
      "  ðŸ“¦ albumentations... âœ“\n",
      "  ðŸ“¦ pandas... âœ“\n",
      "  ðŸ“¦ matplotlib... âœ“\n",
      "  ðŸ“¦ seaborn... âœ“\n",
      "  ðŸ“¦ tqdm... âœ“\n",
      "  ðŸ“¦ scikit-learn... âœ“\n",
      "  ðŸ“¦ pycocotools... âœ“\n",
      "  ðŸ“¦ pyyaml... âœ“\n",
      "  ðŸ“¦ jupyter... âœ“\n",
      "  ðŸ“¦ ipywidgets... âœ“\n",
      "\n",
      "============================================================\n",
      "âœ“ ØªÙ… Ø§Ù„ØªØ«Ø¨ÙŠØª!\n",
      "============================================================\n",
      "âš ï¸ Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Kernel: Kernel â†’ Restart\n",
      "============================================================\n",
      "Configuration:\n",
      "  data_root: ../data\n",
      "  image_size: [1024, 1024]\n",
      "  num_classes: 9\n",
      "  batch_size: 2\n",
      "  num_workers: 0\n",
      "  epochs: 50\n",
      "  learning_rate: 0.0005\n",
      "  device: cuda\n",
      "  early_stop_patience: 7\n",
      "  early_stop_min_delta: 0.001\n",
      "  loss_clip_max: 100.0\n",
      "  loss_skip_threshold: 1000.0\n",
      "  skip_invalid_targets: True\n",
      "  use_medical_augmentation: True\n",
      "  preserve_marker: True\n",
      "  intensity: moderate\n",
      "  train_ratio: 0.7\n",
      "  val_ratio: 0.15\n",
      "  test_ratio: 0.15\n",
      "  checkpoints_dir: ../checkpoints_medical_aug\n",
      "  results_dir: ../results\n",
      "\n",
      "âœ“ Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\"\"\"\n",
    "    try:\n",
    "        print(f\"  ðŸ“¦ {package}...\", end=\" \", flush=True)\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ“\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âš ï¸ (ÙØ´Ù„ - Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„)\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø£: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“¦ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ØªØ«Ø¨ÙŠØª setuptools Ùˆ wheel Ø£ÙˆÙ„Ø§Ù‹ (Ù…Ù‡Ù… Ù„Ù€ Python 3.13)\n",
    "print(\"\\n[1/3] ØªØ«Ø¨ÙŠØª setuptools Ùˆ wheel...\")\n",
    "install_package(\"setuptools\")\n",
    "install_package(\"wheel\")\n",
    "\n",
    "# ØªØ«Ø¨ÙŠØª numpy Ùˆ scipy (Ø¥ØµØ¯Ø§Ø±Ø§Øª ØªØ¯Ø¹Ù… Python 3.13)\n",
    "print(\"\\n[2/3] ØªØ«Ø¨ÙŠØª numpy Ùˆ scipy...\")\n",
    "install_package(\"numpy>=1.26.0\")\n",
    "install_package(\"scipy>=1.11.0\")\n",
    "\n",
    "# ØªØ«Ø¨ÙŠØª Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª (ÙˆØ§Ø­Ø¯Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰)\n",
    "print(\"\\n[3/3] ØªØ«Ø¨ÙŠØª Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\")\n",
    "packages = [\n",
    "    \"torch\", \"torchvision\",\n",
    "    \"opencv-python\", \"Pillow\", \"albumentations\",\n",
    "    \"pandas\", \"matplotlib\", \"seaborn\",\n",
    "    \"tqdm\", \"scikit-learn\", \"pycocotools\",\n",
    "    \"pyyaml\", \"jupyter\", \"ipywidgets\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ ØªÙ… Ø§Ù„ØªØ«Ø¨ÙŠØª!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âš ï¸ Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Kernel: Kernel â†’ Restart\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration - ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ…\n",
    "# ============================================================================\n",
    "\n",
    "import platform\n",
    "\n",
    "CONFIG = {\n",
    "    # Data - Ù…Ø³Ø§Ø±Ø§Øª Ù†Ø³Ø¨ÙŠØ© Ù…Ù† notebooks/ Ø¥Ù„Ù‰ project root\n",
    "    'data_root': '../data',  # Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© (Ù„Ù„ØªØ­Ù‚Ù‚ validation)\n",
    "    # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø²Ø²Ø© ØªÙØ³ØªØ®Ø¯Ù… Ù„Ù„ØªØ¯Ø±ÙŠØ¨ (../data/augmented/)\n",
    "    'image_size': [1024, 1024],\n",
    "    'num_classes': 9,  # 16 wound types + background\n",
    "    'batch_size': 2,\n",
    "    # Ø¹Ù„Ù‰ WindowsØŒ Ø§Ø³ØªØ®Ø¯Ù… num_workers=0 Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ multiprocessing\n",
    "    'num_workers': 0 if platform.system() == 'Windows' else 4,\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.0005,  # Reduced from 0.001 to prevent NaN loss\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Early stopping\n",
    "    'early_stop_patience': 7,\n",
    "    'early_stop_min_delta': 0.001,\n",
    "    \n",
    "    # Training stability\n",
    "    'loss_clip_max': 100.0,\n",
    "    'loss_skip_threshold': 1000.0,\n",
    "    'skip_invalid_targets': True,\n",
    "    \n",
    "    # Medical Augmentation Strategy Settings\n",
    "    'use_medical_augmentation': True,  # Enable comprehensive medical augmentation\n",
    "    'preserve_marker': True,           # Preserve marker geometry (critical for area measurements)\n",
    "    'intensity': \"moderate\",           # Augmentation intensity: \"light\", \"moderate\", \"aggressive\"\n",
    "    \n",
    "    \n",
    "    # Medical Augmentation Strategy Settings\n",
    "    'use_medical_augmentation': True,  # Enable comprehensive medical augmentation\n",
    "    'preserve_marker': True,           # Preserve marker geometry (critical for area measurements)\n",
    "    'intensity': \"moderate\",           # Augmentation intensity: \"light\", \"moderate\", \"aggressive\"\n",
    "    \n",
    "    \n",
    "    # Splits\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    \n",
    "    # Paths - Ù…Ø³Ø§Ø±Ø§Øª Ù†Ø³Ø¨ÙŠØ© Ù…Ù† notebooks/ Ø¥Ù„Ù‰ project root\n",
    "    'checkpoints_dir': '../checkpoints_medical_aug',  # Ù…Ø¬Ù„Ø¯ checkpoints ÙÙŠ Ø§Ù„Ø¬Ø°Ø±\n",
    "    'results_dir': '../results',  # Ù…Ø¬Ù„Ø¯ results ÙÙŠ Ø§Ù„Ø¬Ø°Ø±\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nâœ“ Device: {CONFIG['device']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Part 3: Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Converter function defined!\n",
      "âœ“ Splitter function defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3.1 CVAT to COCO Converter\n",
    "# ============================================================================\n",
    "\n",
    "def convert_cvat_to_coco(data_root: str, output_file: str):\n",
    "    \"\"\"ØªØ­ÙˆÙŠÙ„ ØªØ¹Ù„ÙŠÙ‚Ø§Øª CVAT Ø¥Ù„Ù‰ ØµÙŠØºØ© COCO\"\"\"\n",
    "    \n",
    "    # Convert to absolute path - handle relative paths correctly\n",
    "    current_dir = Path.cwd()\n",
    "    data_root = Path(data_root)\n",
    "    \n",
    "    # Handle relative paths - notebook is in notebooks/ directory\n",
    "    if not data_root.is_absolute():\n",
    "        # If path starts with ../, it's already relative to parent\n",
    "        if str(data_root).startswith('../'):\n",
    "            data_root = current_dir.parent / data_root\n",
    "        # If path is relative and we're in notebooks/, go up one level\n",
    "        elif current_dir.name == 'notebooks':\n",
    "            data_root = current_dir.parent / data_root\n",
    "        else:\n",
    "            data_root = current_dir / data_root\n",
    "    \n",
    "    data_root = data_root.resolve()\n",
    "    \n",
    "    # Check if project.json exists\n",
    "    project_file = data_root / \"project.json\"\n",
    "    if not project_file.exists():\n",
    "        print(\"=\" * 60)\n",
    "        print(\"âŒ Ø®Ø·Ø£: Ù…Ù„Ù project.json ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {project_file}\")\n",
    "        print(f\"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ: {current_dir}\")\n",
    "        print(f\"data_root: {data_root}\")\n",
    "        print(\"\\nðŸ”§ Ø§Ù„Ø­Ù„:\")\n",
    "        print(\"   1. ØªØ£ÙƒØ¯ Ø£Ù† Ù…Ø¬Ù„Ø¯ 'data' Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹\")\n",
    "        print(\"   2. ØªØ£ÙƒØ¯ Ø£Ù† 'data/project.json' Ù…ÙˆØ¬ÙˆØ¯\")\n",
    "        print(\"   3. Ø£Ùˆ Ø¹Ø¯Ù‘Ù„ CONFIG['data_root'] ÙÙŠ Part 2\")\n",
    "        print(\"=\" * 60)\n",
    "        raise FileNotFoundError(f\"project.json not found at {project_file}\")\n",
    "    \n",
    "    # Load project info\n",
    "    with open(project_file, 'r', encoding='utf-8') as f:\n",
    "        project_info = json.load(f)\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_map = {label['name']: idx for idx, label in enumerate(project_info['labels'])}\n",
    "    \n",
    "    # Initialize COCO structure\n",
    "    coco_data = {\n",
    "        'images': [],\n",
    "        'annotations': [],\n",
    "        'categories': [{'id': idx, 'name': name} for name, idx in label_map.items()]\n",
    "    }\n",
    "    \n",
    "    image_id = 0\n",
    "    annotation_id = 0\n",
    "    \n",
    "    # Process all tasks\n",
    "    task_folders = sorted([f for f in data_root.iterdir() if f.is_dir() and f.name.startswith('task_')])\n",
    "    \n",
    "    print(f\"Processing {len(task_folders)} tasks...\")\n",
    "    \n",
    "    for task_folder in tqdm(task_folders):\n",
    "        try:\n",
    "            # Load annotations\n",
    "            with open(task_folder / \"annotations.json\", 'r', encoding='utf-8') as f:\n",
    "                annotations = json.load(f)\n",
    "            \n",
    "            # Get images\n",
    "            data_dir = task_folder / \"data\"\n",
    "            image_files = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*.png'))\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                # Read image to get size\n",
    "                img = cv2.imread(str(img_file))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                \n",
    "                h, w = img.shape[:2]\n",
    "                \n",
    "                # Add image\n",
    "                is_infected = '-not-' not in img_file.name.lower()\n",
    "                coco_data['images'].append({\n",
    "                    'id': image_id,\n",
    "                    'file_name': str(img_file.relative_to(data_root)),\n",
    "                    'width': w,\n",
    "                    'height': h,\n",
    "                    'infection_status': is_infected\n",
    "                })\n",
    "                \n",
    "                # Add annotations for this image (first frame)\n",
    "                if len(annotations) > 0 and 'shapes' in annotations[0]:\n",
    "                    for shape in annotations[0]['shapes']:\n",
    "                        if shape['type'] != 'polygon' or shape['label'] not in label_map:\n",
    "                            continue\n",
    "                        \n",
    "                        # Convert polygon points - handle different formats\n",
    "                        points = shape['points']\n",
    "                        \n",
    "                        # Check if points is a list of lists or a flat list\n",
    "                        if not isinstance(points, list):\n",
    "                            continue\n",
    "                        \n",
    "                        # Handle case where points might be a flat list of floats\n",
    "                        if len(points) > 0 and isinstance(points[0], (int, float)):\n",
    "                            # Flat list: [x1, y1, x2, y2, ...]\n",
    "                            if len(points) % 2 != 0:\n",
    "                                continue\n",
    "                            points = [[points[i], points[i+1]] for i in range(0, len(points), 2)]\n",
    "                        \n",
    "                        # Ensure points is a list of [x, y] pairs\n",
    "                        if not all(isinstance(p, (list, tuple)) and len(p) == 2 for p in points):\n",
    "                            continue\n",
    "                        \n",
    "                        polygon = [coord for point in points for coord in point]\n",
    "                        \n",
    "                        # Calculate bbox\n",
    "                        x_coords = [p[0] for p in points]\n",
    "                        y_coords = [p[1] for p in points]\n",
    "                        x_min, x_max = min(x_coords), max(x_coords)\n",
    "                        y_min, y_max = min(y_coords), max(y_coords)\n",
    "                        bbox = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
    "                        \n",
    "                        # Calculate area\n",
    "                        area = (x_max - x_min) * (y_max - y_min)\n",
    "                        \n",
    "                        coco_data['annotations'].append({\n",
    "                            'id': annotation_id,\n",
    "                            'image_id': image_id,\n",
    "                            'category_id': label_map[shape['label']],\n",
    "                            'segmentation': [polygon],\n",
    "                            'area': area,\n",
    "                            'bbox': bbox,\n",
    "                            'iscrowd': 0\n",
    "                        })\n",
    "                        annotation_id += 1\n",
    "                \n",
    "                image_id += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {task_folder.name}: {e}\")\n",
    "    \n",
    "    # Save COCO file\n",
    "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(coco_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ Done! Saved to {output_file}\")\n",
    "    print(f\"âœ“ Total images: {len(coco_data['images'])}\")\n",
    "    print(f\"âœ“ Total annotations: {len(coco_data['annotations'])}\")\n",
    "    \n",
    "    return coco_data\n",
    "\n",
    "print(\"âœ“ Converter function defined!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3.2 Dataset Splitter\n",
    "# ============================================================================\n",
    "\n",
    "def split_dataset(coco_file: str, output_dir: str, train_r=0.7, val_r=0.15, test_r=0.15):\n",
    "    \"\"\"ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ train/val/test\"\"\"\n",
    "    \n",
    "    with open(coco_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    images = coco_data['images']\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    n = len(images)\n",
    "    n_train = int(n * train_r)\n",
    "    n_val = int(n * val_r)\n",
    "    \n",
    "    splits = {\n",
    "        'train': images[:n_train],\n",
    "        'val': images[n_train:n_train+n_val],\n",
    "        'test': images[n_train+n_val:]\n",
    "    }\n",
    "    \n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for split_name, split_images in splits.items():\n",
    "        split_ids = {img['id'] for img in split_images}\n",
    "        split_anns = [ann for ann in coco_data['annotations'] if ann['image_id'] in split_ids]\n",
    "        \n",
    "        split_data = {\n",
    "            'images': split_images,\n",
    "            'annotations': split_anns,\n",
    "            'categories': coco_data['categories']\n",
    "        }\n",
    "        \n",
    "        output_file = Path(output_dir) / f'{split_name}.json'\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(split_data, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ {split_name}: {len(split_images)} images, {len(split_anns)} annotations\")\n",
    "\n",
    "print(\"âœ“ Splitter function defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸ Part 4: Model Building \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Build model\u001b[39;00m\n\u001b[32m     22\u001b[39m model = build_model(num_classes=CONFIG[\u001b[33m'\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Model built with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m classes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Model moved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1320\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1321\u001b[39m             device,\n\u001b[32m   1322\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1323\u001b[39m             non_blocking,\n\u001b[32m   1324\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1325\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 5.1 Build Model\n",
    "# ============================================================================\n",
    "\n",
    "def build_model(num_classes=17):\n",
    "    \"\"\"Ø¨Ù†Ø§Ø¡ Mask R-CNN model\"\"\"\n",
    "    \n",
    "    # Load pretrained model\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Replace box predictor\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # Replace mask predictor\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_model(num_classes=CONFIG['num_classes'])\n",
    "model.to(CONFIG['device'])\n",
    "print(f\"âœ“ Model built with {CONFIG['num_classes']} classes\")\n",
    "print(f\"âœ“ Model moved to {CONFIG['device']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Import Dataset Utilities from pipeline_utils\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add notebooks directory to path to import pipeline_utils\n",
    "# When running in Jupyter, __file__ is not available, so we use Path.cwd()\n",
    "notebooks_dir = Path.cwd()\n",
    "if notebooks_dir.name != 'notebooks':\n",
    "    # If we're in project root, go to notebooks/\n",
    "    if (notebooks_dir / 'notebooks').exists():\n",
    "        notebooks_dir = notebooks_dir / 'notebooks'\n",
    "    # If we're in a subdirectory, go up until we find notebooks/\n",
    "    else:\n",
    "        parent = notebooks_dir.parent\n",
    "        while parent != parent.parent:\n",
    "            if (parent / 'notebooks').exists():\n",
    "                notebooks_dir = parent / 'notebooks'\n",
    "                break\n",
    "            parent = parent.parent\n",
    "\n",
    "sys.path.insert(0, str(notebooks_dir))\n",
    "\n",
    "from pipeline_utils import create_dataset, make_dataloaders, collate_fn\n",
    "\n",
    "print(\"[OK] Dataset utilities imported from pipeline_utils!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5.2 Create Datasets and DataLoaders using pipeline_utils\n",
    "# ============================================================================\n",
    "\n",
    "# Create datasets using create_dataset from pipeline_utils\n",
    "# This ensures we use the fixed version with proper data validation\n",
    "train_dataset = create_dataset(\n",
    "    root='../data/augmented',  # Path to augmented images\n",
    "    annotation_file='../data/augmented/annotations_augmented.json',\n",
    "    train=True,\n",
    "    image_size=tuple(CONFIG['image_size']),\n",
    "    use_medical_augmentation=CONFIG.get('use_medical_augmentation', True),\n",
    "    preserve_marker=CONFIG.get('preserve_marker', True),\n",
    "    intensity=CONFIG.get('intensity', 'moderate')\n",
    ")\n",
    "\n",
    "# Use original data for validation (real data, not augmented)\n",
    "val_dataset = create_dataset(\n",
    "    root=CONFIG['data_root'],  # '../data' - original data path\n",
    "    annotation_file='../data/splits/val.json',\n",
    "    train=False,\n",
    "    image_size=tuple(CONFIG['image_size']),\n",
    "    use_medical_augmentation=False,  # No augmentation for validation\n",
    "    preserve_marker=True,\n",
    "    intensity='light'\n",
    ")\n",
    "\n",
    "# Create dataloaders using make_dataloaders from pipeline_utils\n",
    "train_loader, val_loader = make_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    shuffle_train=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ Datasets and DataLoaders created!\")\n",
    "print(f\"âœ“ Train samples: {len(train_dataset)}\")\n",
    "print(f\"âœ“ Val samples: {len(val_dataset)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5.4 Setup Optimizer and Scheduler\n",
    "# ============================================================================\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=CONFIG['learning_rate'], momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "print(\"âœ“ Optimizer and scheduler setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Part 5: Start Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Training\n",
      "============================================================\n",
      "\n",
      "Epoch 1/50\n",
      "----------------------------------------\n",
      "Epoch: [1] [0/158] Skipping batch with invalid targets\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m train_loss_dict = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_clip_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss_clip_max\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_skip_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mloss_skip_threshold\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_invalid_targets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mskip_invalid_targets\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m train_loss = train_loss_dict.get(\u001b[33m'\u001b[39m\u001b[33mtotal_loss\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m)  \u001b[38;5;66;03m# Extract total loss\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\notebooks\\train_model.py:221\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, data_loader, device, epoch, print_freq, scaler, scheduler, scheduler_step_per_iter, max_norm, loss_clip_max, loss_skip_threshold, skip_invalid_targets)\u001b[39m\n\u001b[32m    219\u001b[39m         losses = \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict.values())\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     loss_dict = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     losses = \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict.values())\n\u001b[32m    224\u001b[39m loss_value = losses.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m    103\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n\u001b[32m    104\u001b[39m proposals, proposal_losses = \u001b[38;5;28mself\u001b[39m.rpn(images, features, targets)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m detections, detector_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroi_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m detections = \u001b[38;5;28mself\u001b[39m.transform.postprocess(detections, images.image_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[32m    108\u001b[39m losses = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:772\u001b[39m, in \u001b[36mRoIHeads.forward\u001b[39m\u001b[34m(self, features, proposals, image_shapes, targets)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m regression_targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    771\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mregression_targets cannot be None\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m     loss_classifier, loss_box_reg = \u001b[43mfastrcnn_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_regression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    773\u001b[39m     losses = {\u001b[33m\"\u001b[39m\u001b[33mloss_classifier\u001b[39m\u001b[33m\"\u001b[39m: loss_classifier, \u001b[33m\"\u001b[39m\u001b[33mloss_box_reg\u001b[39m\u001b[33m\"\u001b[39m: loss_box_reg}\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:36\u001b[39m, in \u001b[36mfastrcnn_loss\u001b[39m\u001b[34m(class_logits, box_regression, labels, regression_targets)\u001b[39m\n\u001b[32m     31\u001b[39m classification_loss = F.cross_entropy(class_logits, labels)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# get indices that correspond to the regression targets for\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# the corresponding ground truth labels, to be used with\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# advanced indexing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m sampled_pos_inds_subset = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     37\u001b[39m labels_pos = labels[sampled_pos_inds_subset]\n\u001b[32m     38\u001b[39m N, num_classes = class_logits.shape\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Training Loop\n",
    "# ============================================================================\n",
    "\n",
    "from train_model import save_checkpoint, train_one_epoch, validate_one_epoch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improve = 0\n",
    "checkpoints_dir = Path(CONFIG['checkpoints_dir'])\n",
    "checkpoints_dir.mkdir(exist_ok=True)\n",
    "\n",
    "early_stop_patience = CONFIG.get('early_stop_patience', 0)\n",
    "early_stop_min_delta = CONFIG.get('early_stop_min_delta', 0.0)\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{CONFIG['epochs']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss_dict = train_one_epoch(\n",
    "        model, optimizer, train_loader, CONFIG['device'],\n",
    "        epoch=epoch,\n",
    "        loss_clip_max=CONFIG.get('loss_clip_max'),\n",
    "        loss_skip_threshold=CONFIG.get('loss_skip_threshold'),\n",
    "        skip_invalid_targets=CONFIG.get('skip_invalid_targets', True)\n",
    "    )\n",
    "    train_loss = train_loss_dict.get('total_loss', 0.0)  # Extract total loss\n",
    "    \n",
    "    # Validate\n",
    "    val_loss_dict = validate_one_epoch(\n",
    "        model, val_loader, CONFIG['device'],\n",
    "        loss_clip_max=CONFIG.get('loss_clip_max'),\n",
    "        loss_skip_threshold=CONFIG.get('loss_skip_threshold'),\n",
    "        skip_invalid_targets=CONFIG.get('skip_invalid_targets', True)\n",
    "    )\n",
    "    val_loss = val_loss_dict.get('total_loss', 0.0)  # Extract total loss\n",
    "    \n",
    "    # Step scheduler\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Print stats\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Determine best model by val_loss\n",
    "    is_best = val_loss < (best_val_loss - early_stop_min_delta)\n",
    "    if is_best:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improve = 0\n",
    "        print(f\"âœ“ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improve += 1\n",
    "    \n",
    "    # Save last checkpoint + best_model.pth when improved\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'best_metric': best_val_loss,\n",
    "            'best_metric_name': 'val_loss',\n",
    "            'current_metric': val_loss\n",
    "        },\n",
    "        str(checkpoints_dir),\n",
    "        filename='last.pt',\n",
    "        is_best=is_best,\n",
    "        best_metric_name='val_loss',\n",
    "        current_metric=val_loss,\n",
    "        best_filename='best_model.pth'\n",
    "    )\n",
    "    \n",
    "    if early_stop_patience > 0 and epochs_without_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epochs_without_improve} epochs without improvement.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Part 6: Prediction Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7.1 Helper Functions for Prediction\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_wound_area(predictions, marker_class_id=2, marker_size_cm=3.0):\n",
    "    \"\"\"Ø­Ø³Ø§Ø¨ Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø¬Ø±Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ 3Ã—3 Ø³Ù…\"\"\"\n",
    "    \n",
    "    labels = predictions['labels'].cpu().numpy()\n",
    "    masks = predictions['masks'].cpu().numpy()\n",
    "    \n",
    "    # Find marker\n",
    "    marker_idx = np.where(labels == marker_class_id)[0]\n",
    "    \n",
    "    if len(marker_idx) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Get marker mask\n",
    "    marker_mask = masks[marker_idx[0]][0] > 0.5\n",
    "    marker_area_pixels = marker_mask.sum()\n",
    "    \n",
    "    if marker_area_pixels == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Calculate pixel to cm ratio\n",
    "    pixel_to_cm = marker_size_cm / np.sqrt(marker_area_pixels)\n",
    "    \n",
    "    # Find wound (class 0)\n",
    "    wound_idx = np.where(labels == 1)[0]\n",
    "    \n",
    "    if len(wound_idx) == 0:\n",
    "        return None, pixel_to_cm\n",
    "    \n",
    "    # Get wound mask\n",
    "    wound_mask = masks[wound_idx[0]][0] > 0.5\n",
    "    wound_area_pixels = wound_mask.sum()\n",
    "    \n",
    "    # Convert to cmÂ²\n",
    "    wound_area_cm2 = wound_area_pixels * (pixel_to_cm ** 2)\n",
    "    \n",
    "    return wound_area_cm2, pixel_to_cm\n",
    "\n",
    "\n",
    "def detect_infection(predictions, infection_classes=[3, 4, 5, 7, 8]):\n",
    "    \"\"\"ÙƒØ´Ù Ø§Ù„Ø¹Ø¯ÙˆÙ‰ Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©\"\"\"\n",
    "    \n",
    "    labels = predictions['labels'].cpu().numpy()\n",
    "    scores = predictions['scores'].cpu().numpy()\n",
    "    \n",
    "    # Check for infection indicators\n",
    "    infection_detections = []\n",
    "    \n",
    "    for label, score in zip(labels, scores):\n",
    "        if label in infection_classes:\n",
    "            infection_detections.append(float(score))\n",
    "    \n",
    "    if len(infection_detections) > 0:\n",
    "        return True, np.mean(infection_detections)\n",
    "    \n",
    "    return False, 0.0\n",
    "\n",
    "print(\"âœ“ Helper functions defined!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7.2 Prediction Function\n",
    "# ============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(image_path: str, model, device, conf_threshold=0.5):\n",
    "    \"\"\"Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to absolute path if needed\n",
    "    img_path = Path(image_path)\n",
    "    if not img_path.is_absolute():\n",
    "        current_dir = Path.cwd()\n",
    "        if current_dir.name == 'notebooks':\n",
    "            img_path = current_dir.parent / img_path\n",
    "        else:\n",
    "            img_path = current_dir / img_path\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.imread(str(img_path))\n",
    "    \n",
    "    # Check if image loaded successfully\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not load image from: {img_path}\\nPlease check the path and make sure the image exists.\")\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize\n",
    "    image_resized = cv2.resize(image_rgb, tuple(CONFIG['image_size']))\n",
    "    \n",
    "    # To tensor\n",
    "    image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float() / 255.0\n",
    "    \n",
    "    # Normalize\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image_tensor = (image_tensor - mean) / std\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model([image_tensor.to(device)])[0]\n",
    "    \n",
    "    # Filter by confidence\n",
    "    keep = predictions['scores'] >= conf_threshold\n",
    "    filtered = {\n",
    "        'boxes': predictions['boxes'][keep],\n",
    "        'labels': predictions['labels'][keep],\n",
    "        'scores': predictions['scores'][keep],\n",
    "        'masks': predictions['masks'][keep]\n",
    "    }\n",
    "    \n",
    "    # Calculate wound area\n",
    "    wound_area, _ = calculate_wound_area(filtered)\n",
    "    \n",
    "    # Detect infection\n",
    "    has_infection, infection_conf = detect_infection(filtered)\n",
    "    \n",
    "    # Build result\n",
    "    result = {\n",
    "        'image_path': image_path,\n",
    "        'num_detections': len(filtered['labels']),\n",
    "        'wound_area_cm2': float(wound_area) if wound_area else None,\n",
    "        'has_infection': has_infection,\n",
    "        'infection_confidence': float(infection_conf),\n",
    "        'findings': {\n",
    "            'edema': 3 in filtered['labels'].cpu().numpy(),\n",
    "            'hyperemia': 4 in filtered['labels'].cpu().numpy(),\n",
    "            'necrosis': 5 in filtered['labels'].cpu().numpy(),\n",
    "            'granulation': 6 in filtered['labels'].cpu().numpy(),\n",
    "            'fibrin': 7 in filtered['labels'].cpu().numpy(),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result, filtered\n",
    "\n",
    "print(\"âœ“ Prediction function defined!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7.3 Visualization Function\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_prediction(image_path: str, predictions):\n",
    "    \"\"\"Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©\"\"\"\n",
    "    \n",
    "    # Convert to absolute path if needed\n",
    "    img_path = Path(image_path)\n",
    "    if not img_path.is_absolute():\n",
    "        current_dir = Path.cwd()\n",
    "        if current_dir.name == 'notebooks':\n",
    "            img_path = current_dir.parent / img_path\n",
    "        else:\n",
    "            img_path = current_dir / img_path\n",
    "    \n",
    "    image = cv2.imread(str(img_path))\n",
    "    \n",
    "    # Check if image loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"âš ï¸ Warning: Could not load image from: {img_path}\")\n",
    "        return\n",
    "    \n",
    "    image = cv2.resize(image, tuple(CONFIG['image_size']))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    masks = predictions['masks'].cpu().numpy()\n",
    "    labels = predictions['labels'].cpu().numpy()\n",
    "    scores = predictions['scores'].cpu().numpy()\n",
    "    \n",
    "    # Draw masks\n",
    "    for mask, label, score in zip(masks, labels, scores):\n",
    "        mask = (mask[0] > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Random color\n",
    "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "        \n",
    "        # Apply mask\n",
    "        colored_mask = np.zeros_like(image_rgb)\n",
    "        colored_mask[mask > 0] = color\n",
    "        image_rgb = cv2.addWeighted(image_rgb, 0.7, colored_mask, 0.3, 0)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Predictions: {len(labels)} detections\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ“ Visualization function defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Part 7: Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load Model and Predict\n",
    "# ============================================================================\n",
    "\n",
    "# Load better checkpoint (epoch 15 has better results than best_model.pth)\n",
    "# best_model.pth is from epoch 7, but epoch 11 had the best val_loss (3.1325)\n",
    "model_path = Path(CONFIG['checkpoints_dir']) / 'checkpoint_epoch_15.pth'\n",
    "\n",
    "if model_path.exists():\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    checkpoint = torch.load(model_path, map_location=CONFIG['device'], weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(\"âœ“ Model loaded successfully!\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"  - Epoch: {checkpoint['epoch']}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Model not found! Please train the model first (Part 6)\")\n",
    "\n",
    "# ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø± Ø¥Ù„Ù‰ ØµÙˆØ±Ø© ØªØ±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„ÙŠÙ‡Ø§\n",
    "image_path = 'data/task_0/data/task_0_image_001.jpg'  # â¬…ï¸ Ø¹Ø¯Ù‘Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³Ø§Ø±\n",
    "\n",
    "# Lower confidence threshold for better recall (0.3 instead of 0.5)\n",
    "CONF_THRESHOLD = 0.3\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Wound Detection - Prediction\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nImage: {image_path}\")\n",
    "print(f\"Confidence Threshold: {CONF_THRESHOLD}\")\n",
    "\n",
    "# Predict with lower threshold\n",
    "print(\"\\nPredicting...\")\n",
    "result, predictions = predict_image(image_path, model, CONFIG['device'], conf_threshold=CONF_THRESHOLD)\n",
    "\n",
    "# Print result\n",
    "print(\"\\nResults:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Detections: {result['num_detections']}\")\n",
    "print(f\"Wound Area: {result['wound_area_cm2']} cmÂ²\" if result['wound_area_cm2'] else \"Wound Area: N/A\")\n",
    "print(f\"Infection: {'YES' if result['has_infection'] else 'NO'} (confidence: {result['infection_confidence']:.2f})\")\n",
    "print(\"\\nFindings:\")\n",
    "for finding, present in result['findings'].items():\n",
    "    print(f\"  {finding}: {'âœ“' if present else 'âœ—'}\")\n",
    "\n",
    "# Visualize\n",
    "print(\"\\nVisualizing...\")\n",
    "visualize_prediction(image_path, predictions)\n",
    "\n",
    "# Save result\n",
    "output_dir = Path(CONFIG['results_dir'])\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "result_file = output_dir / f\"{Path(image_path).stem}_result.json\"\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "print(f\"\\nâœ“ Result saved: {result_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run with out train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Quick Start: Load Everything Automatically\n",
    "# ============================================================================\n",
    "# Run this cell first when opening a new notebook session!\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch Vision\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ“ Libraries imported!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load CONFIG\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    'num_classes': 9,  # Background + 16 classes\n",
    "    'batch_size': 2,\n",
    "    'num_workers': 2,\n",
    "    'learning_rate': 0.0005,\n",
    "    'num_epochs': 50,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'checkpoints_dir': '../checkpoints_medical_aug',\n",
    "}\n",
    "\n",
    "print(\"âœ“ CONFIG loaded!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Build Model\n",
    "# ============================================================================\n",
    "def build_model(num_classes=17):\n",
    "    \"\"\"Ø¨Ù†Ø§Ø¡ Mask R-CNN model\"\"\"\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
    "    return model\n",
    "\n",
    "model = build_model(num_classes=CONFIG['num_classes'])\n",
    "model.to(CONFIG['device'])\n",
    "print(\"âœ“ Model built!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Trained Model (if exists)\n",
    "# ============================================================================\n",
    "# Find project root\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir\n",
    "for parent in current_dir.parents:\n",
    "    if (parent / 'README.md').exists() or (parent / 'requirements.txt').exists():\n",
    "        project_root = parent\n",
    "        break\n",
    "project_root = Path(project_root).resolve()\n",
    "\n",
    "# Resolve checkpoint path\n",
    "checkpoints_dir = CONFIG['checkpoints_dir']\n",
    "if checkpoints_dir.startswith('../') or checkpoints_dir.startswith('..\\\\'):\n",
    "    checkpoints_dir = checkpoints_dir.replace('../', '').replace('..\\\\', '')\n",
    "checkpoints_dir = project_root / checkpoints_dir\n",
    "\n",
    "# Use checkpoint_epoch_15 instead of best_model.pth (epoch 15 is closer to best val_loss at epoch 11)\n",
    "model_path = checkpoints_dir / 'checkpoint_epoch_15.pth'\n",
    "\n",
    "if model_path.exists():\n",
    "    print(f\"âœ“ Loading trained model from: {model_path}\")\n",
    "    # weights_only=False is needed to load optimizer_state_dict, epoch, val_loss, etc.\n",
    "    checkpoint = torch.load(model_path, map_location=CONFIG['device'], weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(\"âœ“ Trained model loaded successfully!\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"  - Epoch: {checkpoint['epoch']}\")\n",
    "    if 'val_loss' in checkpoint:\n",
    "        print(f\"  - Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  No trained model found at: {model_path}\")\n",
    "    print(\"   Run Part 6 (Training) to train a model first.\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Quick Start Complete! You can now use the model for prediction.\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_cuda (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
