{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Wound Infection Detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Part 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "âœ“ All libraries imported successfully!\n",
            "============================================================\n",
            "PyTorch: 2.9.1+cpu\n",
            "NumPy: 2.2.6\n",
            "CUDA: False\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Import all required libraries\n",
        "# ============================================================================\n",
        "\n",
        "try:\n",
        "    import json\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "    from pathlib import Path\n",
        "    from typing import Dict, List, Tuple\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    from tqdm import tqdm\n",
        "    import random\n",
        "    import yaml\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    # PyTorch Vision\n",
        "    import torchvision\n",
        "    from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "    from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"âœ“ All libraries imported successfully!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    print(f\"NumPy: {np.__version__}\")\n",
        "    print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "except ValueError as e:\n",
        "    if \"numpy.dtype size changed\" in str(e):\n",
        "        print(\"=\" * 60)\n",
        "        print(\"âŒ Ø®Ø·Ø£: ØªØ¹Ø§Ø±Ø¶ Ø¨ÙŠÙ† numpy Ùˆ scipy\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\nðŸ”§ Ø§Ù„Ø­Ù„:\")\n",
        "        print(\"   1. Ø´ØºÙ‘Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙˆØ§Ù…Ø± ÙÙŠ Terminal:\")\n",
        "        print(\"      pip install --upgrade --force-reinstall numpy scipy\")\n",
        "        print(\"   2. Ø£Ùˆ Ø´ØºÙ‘Ù„ Part 0 Ù…Ø±Ø© Ø£Ø®Ø±Ù‰\")\n",
        "        print(\"   3. Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Kernel: Kernel â†’ Restart\")\n",
        "        print(\"=\" * 60)\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "        \n",
        "except ImportError as e:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Ø§Ù„Ø®Ø·Ø£: {e}\")\n",
        "    print(\"\\nðŸ”§ Ø§Ù„Ø­Ù„:\")\n",
        "    print(\"   1. Ø´ØºÙ‘Ù„ Part 0 Ø£ÙˆÙ„Ø§Ù‹ (ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª)\")\n",
        "    print(\"   2. Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Kernel: Kernel â†’ Restart\")\n",
        "    print(\"=\" * 60)\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Part 2: Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“¦ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\n",
            "============================================================\n",
            "\n",
            "[1/3] ØªØ«Ø¨ÙŠØª setuptools Ùˆ wheel...\n",
            "  ðŸ“¦ setuptools... âœ“\n",
            "  ðŸ“¦ wheel... âœ“\n",
            "\n",
            "[2/3] ØªØ«Ø¨ÙŠØª numpy Ùˆ scipy...\n",
            "  ðŸ“¦ numpy>=1.26.0... âœ“\n",
            "  ðŸ“¦ scipy>=1.11.0... âœ“\n",
            "\n",
            "[3/3] ØªØ«Ø¨ÙŠØª Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\n",
            "  ðŸ“¦ torch... âœ“\n",
            "  ðŸ“¦ torchvision... âœ“\n",
            "  ðŸ“¦ opencv-python... âœ“\n",
            "  ðŸ“¦ Pillow... âœ“\n",
            "  ðŸ“¦ albumentations... âœ“\n",
            "  ðŸ“¦ pandas... âœ“\n",
            "  ðŸ“¦ matplotlib... âœ“\n",
            "  ðŸ“¦ seaborn... âœ“\n",
            "  ðŸ“¦ tqdm... âœ“\n",
            "  ðŸ“¦ scikit-learn... âœ“\n",
            "  ðŸ“¦ pycocotools... âœ“\n",
            "  ðŸ“¦ pyyaml... âœ“\n",
            "  ðŸ“¦ jupyter... âœ“\n",
            "  ðŸ“¦ ipywidgets... âœ“\n",
            "\n",
            "============================================================\n",
            "âœ“ ØªÙ… Ø§Ù„ØªØ«Ø¨ÙŠØª!\n",
            "============================================================\n",
            "âš ï¸ Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Kernel: Kernel â†’ Restart\n",
            "============================================================\n",
            "Configuration:\n",
            "  data_root: ../data\n",
            "  image_size: [1024, 1024]\n",
            "  num_classes: 17\n",
            "  batch_size: 2\n",
            "  num_workers: 0\n",
            "  epochs: 50\n",
            "  learning_rate: 0.001\n",
            "  device: cpu\n",
            "  train_ratio: 0.7\n",
            "  val_ratio: 0.15\n",
            "  test_ratio: 0.15\n",
            "  checkpoints_dir: ../checkpoints_medical_aug\n",
            "  results_dir: ../results\n",
            "\n",
            "âœ“ Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\"\"\"\n",
        "    try:\n",
        "        print(f\"  ðŸ“¦ {package}...\", end=\" \", flush=True)\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", package],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=False\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(\"âœ“\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"âš ï¸ (ÙØ´Ù„ - Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„)\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø£: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“¦ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ØªØ«Ø¨ÙŠØª setuptools Ùˆ wheel Ø£ÙˆÙ„Ø§Ù‹ (Ù…Ù‡Ù… Ù„Ù€ Python 3.13)\n",
        "print(\"\\n[1/3] ØªØ«Ø¨ÙŠØª setuptools Ùˆ wheel...\")\n",
        "install_package(\"setuptools\")\n",
        "install_package(\"wheel\")\n",
        "\n",
        "# ØªØ«Ø¨ÙŠØª numpy Ùˆ scipy (Ø¥ØµØ¯Ø§Ø±Ø§Øª ØªØ¯Ø¹Ù… Python 3.13)\n",
        "print(\"\\n[2/3] ØªØ«Ø¨ÙŠØª numpy Ùˆ scipy...\")\n",
        "install_package(\"numpy>=1.26.0\")\n",
        "install_package(\"scipy>=1.11.0\")\n",
        "\n",
        "# ØªØ«Ø¨ÙŠØª Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª (ÙˆØ§Ø­Ø¯Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰)\n",
        "print(\"\\n[3/3] ØªØ«Ø¨ÙŠØª Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\")\n",
        "packages = [\n",
        "    \"torch\", \"torchvision\",\n",
        "    \"opencv-python\", \"Pillow\", \"albumentations\",\n",
        "    \"pandas\", \"matplotlib\", \"seaborn\",\n",
        "    \"tqdm\", \"scikit-learn\", \"pycocotools\",\n",
        "    \"pyyaml\", \"jupyter\", \"ipywidgets\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    install_package(package)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ“ ØªÙ… Ø§Ù„ØªØ«Ø¨ÙŠØª!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"âš ï¸ Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Kernel: Kernel â†’ Restart\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration - ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ…\n",
        "# ============================================================================\n",
        "\n",
        "import platform\n",
        "\n",
        "CONFIG = {\n",
        "    # Data - Ù…Ø³Ø§Ø±Ø§Øª Ù†Ø³Ø¨ÙŠØ© Ù…Ù† notebooks/ Ø¥Ù„Ù‰ project root\n",
        "    'data_root': '../data',  # Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ\n",
        "    'image_size': [1024, 1024],\n",
        "    'num_classes': 17,  # 16 wound types + background\n",
        "    'batch_size': 2,\n",
        "    # Ø¹Ù„Ù‰ WindowsØŒ Ø§Ø³ØªØ®Ø¯Ù… num_workers=0 Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ multiprocessing\n",
        "    'num_workers': 0 if platform.system() == 'Windows' else 4,\n",
        "    \n",
        "    # Training\n",
        "    'epochs': 50,\n",
        "    'learning_rate': 0.001,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "    # Splits\n",
        "    'train_ratio': 0.7,\n",
        "    'val_ratio': 0.15,\n",
        "    'test_ratio': 0.15,\n",
        "    \n",
        "    # Paths - Ù…Ø³Ø§Ø±Ø§Øª Ù†Ø³Ø¨ÙŠØ© Ù…Ù† notebooks/ Ø¥Ù„Ù‰ project root\n",
        "    'checkpoints_dir': '../checkpoints_medical_aug',  # Ù…Ø¬Ù„Ø¯ checkpoints ÙÙŠ Ø§Ù„Ø¬Ø°Ø±\n",
        "    'results_dir': '../results',  # Ù…Ø¬Ù„Ø¯ results ÙÙŠ Ø§Ù„Ø¬Ø°Ø±\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"\\nâœ“ Device: {CONFIG['device']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Part 3: Data Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Converter function defined!\n",
            "âœ“ Splitter function defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 3.1 CVAT to COCO Converter\n",
        "# ============================================================================\n",
        "\n",
        "def convert_cvat_to_coco(data_root: str, output_file: str):\n",
        "    \"\"\"ØªØ­ÙˆÙŠÙ„ ØªØ¹Ù„ÙŠÙ‚Ø§Øª CVAT Ø¥Ù„Ù‰ ØµÙŠØºØ© COCO\"\"\"\n",
        "    \n",
        "    # Convert to absolute path - handle relative paths correctly\n",
        "    current_dir = Path.cwd()\n",
        "    data_root = Path(data_root)\n",
        "    \n",
        "    # Handle relative paths - notebook is in notebooks/ directory\n",
        "    if not data_root.is_absolute():\n",
        "        # If path starts with ../, it's already relative to parent\n",
        "        if str(data_root).startswith('../'):\n",
        "            data_root = current_dir.parent / data_root\n",
        "        # If path is relative and we're in notebooks/, go up one level\n",
        "        elif current_dir.name == 'notebooks':\n",
        "            data_root = current_dir.parent / data_root\n",
        "        else:\n",
        "            data_root = current_dir / data_root\n",
        "    \n",
        "    data_root = data_root.resolve()\n",
        "    \n",
        "    # Check if project.json exists\n",
        "    project_file = data_root / \"project.json\"\n",
        "    if not project_file.exists():\n",
        "        print(\"=\" * 60)\n",
        "        print(\"âŒ Ø®Ø·Ø£: Ù…Ù„Ù project.json ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {project_file}\")\n",
        "        print(f\"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ: {current_dir}\")\n",
        "        print(f\"data_root: {data_root}\")\n",
        "        print(\"\\nðŸ”§ Ø§Ù„Ø­Ù„:\")\n",
        "        print(\"   1. ØªØ£ÙƒØ¯ Ø£Ù† Ù…Ø¬Ù„Ø¯ 'data' Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹\")\n",
        "        print(\"   2. ØªØ£ÙƒØ¯ Ø£Ù† 'data/project.json' Ù…ÙˆØ¬ÙˆØ¯\")\n",
        "        print(\"   3. Ø£Ùˆ Ø¹Ø¯Ù‘Ù„ CONFIG['data_root'] ÙÙŠ Part 2\")\n",
        "        print(\"=\" * 60)\n",
        "        raise FileNotFoundError(f\"project.json not found at {project_file}\")\n",
        "    \n",
        "    # Load project info\n",
        "    with open(project_file, 'r', encoding='utf-8') as f:\n",
        "        project_info = json.load(f)\n",
        "    \n",
        "    # Create label mapping\n",
        "    label_map = {label['name']: idx for idx, label in enumerate(project_info['labels'])}\n",
        "    \n",
        "    # Initialize COCO structure\n",
        "    coco_data = {\n",
        "        'images': [],\n",
        "        'annotations': [],\n",
        "        'categories': [{'id': idx, 'name': name} for name, idx in label_map.items()]\n",
        "    }\n",
        "    \n",
        "    image_id = 0\n",
        "    annotation_id = 0\n",
        "    \n",
        "    # Process all tasks\n",
        "    task_folders = sorted([f for f in data_root.iterdir() if f.is_dir() and f.name.startswith('task_')])\n",
        "    \n",
        "    print(f\"Processing {len(task_folders)} tasks...\")\n",
        "    \n",
        "    for task_folder in tqdm(task_folders):\n",
        "        try:\n",
        "            # Load annotations\n",
        "            with open(task_folder / \"annotations.json\", 'r', encoding='utf-8') as f:\n",
        "                annotations = json.load(f)\n",
        "            \n",
        "            # Get images\n",
        "            data_dir = task_folder / \"data\"\n",
        "            image_files = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*.png'))\n",
        "            \n",
        "            for img_file in image_files:\n",
        "                # Read image to get size\n",
        "                img = cv2.imread(str(img_file))\n",
        "                if img is None:\n",
        "                    continue\n",
        "                \n",
        "                h, w = img.shape[:2]\n",
        "                \n",
        "                # Add image\n",
        "                is_infected = '-not-' not in img_file.name.lower()\n",
        "                coco_data['images'].append({\n",
        "                    'id': image_id,\n",
        "                    'file_name': str(img_file.relative_to(data_root)),\n",
        "                    'width': w,\n",
        "                    'height': h,\n",
        "                    'infection_status': is_infected\n",
        "                })\n",
        "                \n",
        "                # Add annotations for this image (first frame)\n",
        "                if len(annotations) > 0 and 'shapes' in annotations[0]:\n",
        "                    for shape in annotations[0]['shapes']:\n",
        "                        if shape['type'] != 'polygon' or shape['label'] not in label_map:\n",
        "                            continue\n",
        "                        \n",
        "                        # Convert polygon points - handle different formats\n",
        "                        points = shape['points']\n",
        "                        \n",
        "                        # Check if points is a list of lists or a flat list\n",
        "                        if not isinstance(points, list):\n",
        "                            continue\n",
        "                        \n",
        "                        # Handle case where points might be a flat list of floats\n",
        "                        if len(points) > 0 and isinstance(points[0], (int, float)):\n",
        "                            # Flat list: [x1, y1, x2, y2, ...]\n",
        "                            if len(points) % 2 != 0:\n",
        "                                continue\n",
        "                            points = [[points[i], points[i+1]] for i in range(0, len(points), 2)]\n",
        "                        \n",
        "                        # Ensure points is a list of [x, y] pairs\n",
        "                        if not all(isinstance(p, (list, tuple)) and len(p) == 2 for p in points):\n",
        "                            continue\n",
        "                        \n",
        "                        polygon = [coord for point in points for coord in point]\n",
        "                        \n",
        "                        # Calculate bbox\n",
        "                        x_coords = [p[0] for p in points]\n",
        "                        y_coords = [p[1] for p in points]\n",
        "                        x_min, x_max = min(x_coords), max(x_coords)\n",
        "                        y_min, y_max = min(y_coords), max(y_coords)\n",
        "                        bbox = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "                        \n",
        "                        # Calculate area\n",
        "                        area = (x_max - x_min) * (y_max - y_min)\n",
        "                        \n",
        "                        coco_data['annotations'].append({\n",
        "                            'id': annotation_id,\n",
        "                            'image_id': image_id,\n",
        "                            'category_id': label_map[shape['label']],\n",
        "                            'segmentation': [polygon],\n",
        "                            'area': area,\n",
        "                            'bbox': bbox,\n",
        "                            'iscrowd': 0\n",
        "                        })\n",
        "                        annotation_id += 1\n",
        "                \n",
        "                image_id += 1\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {task_folder.name}: {e}\")\n",
        "    \n",
        "    # Save COCO file\n",
        "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(coco_data, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nâœ“ Done! Saved to {output_file}\")\n",
        "    print(f\"âœ“ Total images: {len(coco_data['images'])}\")\n",
        "    print(f\"âœ“ Total annotations: {len(coco_data['annotations'])}\")\n",
        "    \n",
        "    return coco_data\n",
        "\n",
        "print(\"âœ“ Converter function defined!\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3.2 Dataset Splitter\n",
        "# ============================================================================\n",
        "\n",
        "def split_dataset(coco_file: str, output_dir: str, train_r=0.7, val_r=0.15, test_r=0.15):\n",
        "    \"\"\"ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ train/val/test\"\"\"\n",
        "    \n",
        "    with open(coco_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "    \n",
        "    images = coco_data['images']\n",
        "    random.shuffle(images)\n",
        "    \n",
        "    n = len(images)\n",
        "    n_train = int(n * train_r)\n",
        "    n_val = int(n * val_r)\n",
        "    \n",
        "    splits = {\n",
        "        'train': images[:n_train],\n",
        "        'val': images[n_train:n_train+n_val],\n",
        "        'test': images[n_train+n_val:]\n",
        "    }\n",
        "    \n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    for split_name, split_images in splits.items():\n",
        "        split_ids = {img['id'] for img in split_images}\n",
        "        split_anns = [ann for ann in coco_data['annotations'] if ann['image_id'] in split_ids]\n",
        "        \n",
        "        split_data = {\n",
        "            'images': split_images,\n",
        "            'annotations': split_anns,\n",
        "            'categories': coco_data['categories']\n",
        "        }\n",
        "        \n",
        "        output_file = Path(output_dir) / f'{split_name}.json'\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(split_data, f, indent=2)\n",
        "        \n",
        "        print(f\"âœ“ {split_name}: {len(split_images)} images, {len(split_anns)} annotations\")\n",
        "\n",
        "print(\"âœ“ Splitter function defined!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ‹ï¸ Part 5: Model Building & Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Model built with 17 classes\n",
            "âœ“ Model moved to cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5.1 Build Model\n",
        "# ============================================================================\n",
        "\n",
        "def build_model(num_classes=17):\n",
        "    \"\"\"Ø¨Ù†Ø§Ø¡ Mask R-CNN model\"\"\"\n",
        "    \n",
        "    # Load pretrained model\n",
        "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
        "    \n",
        "    # Replace box predictor\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    \n",
        "    # Replace mask predictor\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = build_model(num_classes=CONFIG['num_classes'])\n",
        "model.to(CONFIG['device'])\n",
        "print(f\"âœ“ Model built with {CONFIG['num_classes']} classes\")\n",
        "print(f\"âœ“ Model moved to {CONFIG['device']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Training functions defined!\n",
            "âœ“ Dataset class defined!\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "COCO annotation file not found: E:\\GitHub\\data\\splits\\train.json\nProject root: E:\\GitHub\\Wound-infection-detection-model\nCurrent dir: e:\\GitHub\\Wound-infection-detection-model\\notebooks\nOriginal path: E:\\GitHub\\Wound-infection-detection-model\\..\\data\\splits\\train.json\nPlease check the path and ensure the file exists.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 354\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Dataset class defined!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# 5.2 Create Datasets and DataLoaders (moved here after class definition)\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m    352\u001b[39m \n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m train_dataset = \u001b[43mWoundDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/splits/train.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata_root\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    359\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m val_dataset = WoundDataset(\n\u001b[32m    362\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m../data/splits/val.json\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    363\u001b[39m     CONFIG[\u001b[33m'\u001b[39m\u001b[33mdata_root\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    364\u001b[39m     \u001b[38;5;28mtuple\u001b[39m(CONFIG[\u001b[33m'\u001b[39m\u001b[33mimage_size\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m    365\u001b[39m     is_train=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    366\u001b[39m )\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# Create dataloaders\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mWoundDataset.__init__\u001b[39m\u001b[34m(self, coco_file, data_root, image_size, is_train)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Verify file exists before trying to open\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(coco_file).exists():\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCOCO annotation file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoco_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProject root: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    165\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent dir: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoco_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease check the path and ensure the file exists.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28mself\u001b[39m.image_size = image_size\n\u001b[32m    170\u001b[39m \u001b[38;5;28mself\u001b[39m.is_train = is_train\n",
            "\u001b[31mFileNotFoundError\u001b[39m: COCO annotation file not found: E:\\GitHub\\data\\splits\\train.json\nProject root: E:\\GitHub\\Wound-infection-detection-model\nCurrent dir: e:\\GitHub\\Wound-infection-detection-model\\notebooks\nOriginal path: E:\\GitHub\\Wound-infection-detection-model\\..\\data\\splits\\train.json\nPlease check the path and ensure the file exists."
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5.2 Create Datasets and DataLoaders\n",
        "# ============================================================================\n",
        "\n",
        "# Note: WoundDataset class is defined later in this cell\n",
        "# The dataset creation code is moved to the end of this cell, after the class definition\n",
        "# Make sure to run this entire cell from the beginning\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5.3 Training Functions\n",
        "# ============================================================================\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device):\n",
        "    \"\"\"ØªØ¯Ø±ÙŠØ¨ epoch ÙˆØ§Ø­Ø¯\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    pbar = tqdm(data_loader, desc=\"Training\")\n",
        "    \n",
        "    for images, targets in pbar:\n",
        "        # Move to device\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        # Forward\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += losses.item()\n",
        "        pbar.set_postfix({'loss': f'{losses.item():.4f}'})\n",
        "    \n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, data_loader, device):\n",
        "    \"\"\"Validation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    pbar = tqdm(data_loader, desc=\"Validation\")\n",
        "    \n",
        "    for images, targets in pbar:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        # Get loss\n",
        "        model.train()  # Need to be in train mode to get loss\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        model.eval()\n",
        "        \n",
        "        total_loss += losses.item()\n",
        "        pbar.set_postfix({'loss': f'{losses.item():.4f}'})\n",
        "    \n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "print(\"âœ“ Training functions defined!\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3.3 PyTorch Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class WoundDataset(Dataset):\n",
        "    \"\"\"Dataset Ø¨Ø³ÙŠØ· Ù„Ù„Ø¬Ø±ÙˆØ­\"\"\"\n",
        "    \n",
        "    def __init__(self, coco_file: str, data_root: str, image_size=(1024, 1024), is_train=True):\n",
        "        # Find project root by looking for README.md or requirements.txt\n",
        "        # Start from current directory and go up until we find project root\n",
        "        current_dir = Path.cwd()\n",
        "        project_root = None\n",
        "        \n",
        "        # Strategy 1: Try to find project root by looking for README.md or requirements.txt\n",
        "        # Start from current_dir and go up\n",
        "        search_dir = current_dir\n",
        "        max_levels = 10  # Search up to 10 levels\n",
        "        for level in range(max_levels):\n",
        "            if (search_dir / 'README.md').exists() or (search_dir / 'requirements.txt').exists():\n",
        "                project_root = search_dir\n",
        "                break\n",
        "            # Check if we've reached filesystem root\n",
        "            parent = search_dir.parent\n",
        "            if parent == search_dir:  # Reached filesystem root\n",
        "                break\n",
        "            search_dir = parent\n",
        "        \n",
        "        # Strategy 2: If project root not found, try to find 'notebooks' directory\n",
        "        if project_root is None:\n",
        "            search_dir = current_dir\n",
        "            for _ in range(10):\n",
        "                # If we find a 'notebooks' directory, its parent is likely the project root\n",
        "                if (search_dir / 'notebooks').exists() and (search_dir / 'notebooks').is_dir():\n",
        "                    project_root = search_dir\n",
        "                    break\n",
        "                # Also check if current directory is 'notebooks'\n",
        "                if search_dir.name == 'notebooks':\n",
        "                    potential_root = search_dir.parent\n",
        "                    if (potential_root / 'README.md').exists() or (potential_root / 'requirements.txt').exists():\n",
        "                        project_root = potential_root\n",
        "                        break\n",
        "                parent = search_dir.parent\n",
        "                if parent == search_dir:\n",
        "                    break\n",
        "                search_dir = parent\n",
        "        \n",
        "        # Strategy 3: If still not found, try to find 'data' directory with 'splits' subdirectory\n",
        "        if project_root is None:\n",
        "            search_dir = current_dir\n",
        "            for _ in range(10):\n",
        "                if (search_dir / 'data' / 'splits').exists() and (search_dir / 'data' / 'splits').is_dir():\n",
        "                    project_root = search_dir\n",
        "                    break\n",
        "                parent = search_dir.parent\n",
        "                if parent == search_dir:\n",
        "                    break\n",
        "                search_dir = parent\n",
        "        \n",
        "        # Fallback: use current_dir\n",
        "        if project_root is None:\n",
        "            project_root = current_dir\n",
        "        \n",
        "        # Ensure project_root is absolute and resolved\n",
        "        project_root = Path(project_root).resolve()\n",
        "        \n",
        "        # Resolve data_root path\n",
        "        data_root = Path(data_root)\n",
        "        if not data_root.is_absolute():\n",
        "            if str(data_root).startswith('../'):\n",
        "                # Remove '../' prefix and resolve from project root\n",
        "                relative_path = str(data_root)[3:]  # Remove '../'\n",
        "                data_root = project_root / relative_path\n",
        "            else:\n",
        "                data_root = project_root / data_root\n",
        "        \n",
        "        self.data_root = data_root.resolve()\n",
        "        \n",
        "        # Handle coco_file path - resolve relative to project root\n",
        "        # project_root is already resolved to absolute path\n",
        "        coco_file_str = str(coco_file)\n",
        "        if not Path(coco_file).is_absolute():\n",
        "            if coco_file_str.startswith('../'):\n",
        "                # Remove '../' prefix and build path relative to project_root\n",
        "                relative_path = coco_file_str[3:]  # Remove '../'\n",
        "                # Build absolute path directly\n",
        "                coco_file_path = project_root / relative_path\n",
        "            else:\n",
        "                # Path is relative but doesn't start with ../\n",
        "                coco_file_path = project_root / coco_file_str\n",
        "        else:\n",
        "            # Path is already absolute\n",
        "            coco_file_path = Path(coco_file)\n",
        "        \n",
        "        # Resolve to absolute path (should already be absolute, but resolve() handles any remaining .. or .)\n",
        "        coco_file = str(coco_file_path.resolve())\n",
        "        \n",
        "        # Verify file exists before trying to open\n",
        "        if not Path(coco_file).exists():\n",
        "            # Additional debugging: check if the path components are correct\n",
        "            debug_info = (\n",
        "                f\"COCO annotation file not found: {coco_file}\\n\"\n",
        "                f\"Project root: {project_root}\\n\"\n",
        "                f\"Project root type: {type(project_root)}\\n\"\n",
        "                f\"Current dir: {current_dir}\\n\"\n",
        "                f\"Original coco_file parameter: {coco_file}\\n\"\n",
        "                f\"Relative path extracted: {relative_path if 'relative_path' in locals() else 'N/A'}\\n\"\n",
        "                f\"Path before resolve: {coco_file_path}\\n\"\n",
        "                f\"Path after resolve: {coco_file}\\n\"\n",
        "                f\"Please check the path and ensure the file exists.\"\n",
        "            )\n",
        "            raise FileNotFoundError(debug_info)\n",
        "        self.image_size = image_size\n",
        "        self.is_train = is_train\n",
        "        \n",
        "        # Load COCO data\n",
        "        with open(coco_file, 'r', encoding='utf-8') as f:\n",
        "            self.coco = json.load(f)\n",
        "        \n",
        "        self.images = self.coco['images']\n",
        "        \n",
        "        # Create annotation index\n",
        "        self.img_to_anns = {}\n",
        "        for ann in self.coco['annotations']:\n",
        "            img_id = ann['image_id']\n",
        "            if img_id not in self.img_to_anns:\n",
        "                self.img_to_anns[img_id] = []\n",
        "            self.img_to_anns[img_id].append(ann)\n",
        "        \n",
        "        # Setup transforms\n",
        "        if self.is_train:\n",
        "            # Transform with bbox support (for images with annotations)\n",
        "            self.transform_with_bbox = A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "                A.Resize(*self.image_size),\n",
        "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ToTensorV2()\n",
        "            ], bbox_params=A.BboxParams(format='coco', label_fields=['labels']))\n",
        "            # Transform without bbox (for images without annotations)\n",
        "            self.transform_no_bbox = A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "                A.Resize(*self.image_size),\n",
        "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "            self.transform = None  # Will use transform_with_bbox or transform_no_bbox in __getitem__\n",
        "        else:\n",
        "            # For validation, no bbox_params needed\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(*self.image_size),\n",
        "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "            self.transform_with_bbox = None\n",
        "            self.transform_no_bbox = None\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.images[idx]\n",
        "        img_id = img_info['id']\n",
        "        \n",
        "        # Load image\n",
        "        img_path = self.data_root / img_info['file_name']\n",
        "        image = cv2.imread(str(img_path))\n",
        "        \n",
        "        # Check if image loaded successfully\n",
        "        if image is None:\n",
        "            print(f\"âš ï¸ Warning: Could not load image: {img_path}\")\n",
        "            # Create a dummy black image as fallback\n",
        "            image = np.zeros((img_info['height'], img_info['width'], 3), dtype=np.uint8)\n",
        "        else:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Get annotations\n",
        "        anns = self.img_to_anns.get(img_id, [])\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "        \n",
        "        # Get image dimensions for normalization\n",
        "        img_h, img_w = image.shape[:2]\n",
        "        \n",
        "        for ann in anns:\n",
        "            # COCO bbox format: [x, y, width, height] in pixels\n",
        "            bbox = ann['bbox']\n",
        "            # Normalize to [0, 1] range for albumentations\n",
        "            normalized_bbox = [\n",
        "                bbox[0] / img_w,  # x\n",
        "                bbox[1] / img_h,  # y\n",
        "                bbox[2] / img_w,  # width\n",
        "                bbox[3] / img_h   # height\n",
        "            ]\n",
        "            boxes.append(normalized_bbox)\n",
        "            labels.append(ann['category_id'])\n",
        "            \n",
        "            # Create mask from polygon\n",
        "            mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
        "            for seg in ann['segmentation']:\n",
        "                poly = np.array(seg).reshape(-1, 2).astype(np.int32)\n",
        "                cv2.fillPoly(mask, [poly], 1)\n",
        "            masks.append(mask)\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.is_train:\n",
        "            if len(boxes) > 0:\n",
        "                # Use transform with bbox support\n",
        "                transformed = self.transform_with_bbox(\n",
        "                    image=image,\n",
        "                    bboxes=boxes,\n",
        "                    labels=labels,\n",
        "                    masks=masks\n",
        "                )\n",
        "                image = transformed['image']\n",
        "                boxes = transformed.get('bboxes', [])\n",
        "                labels = transformed.get('labels', [])\n",
        "                masks = transformed.get('masks', [])\n",
        "                \n",
        "                # Convert normalized bboxes back to pixel coordinates\n",
        "                # Get transformed image size\n",
        "                if isinstance(image, torch.Tensor):\n",
        "                    _, new_h, new_w = image.shape\n",
        "                else:\n",
        "                    new_h, new_w = image.shape[:2]\n",
        "                \n",
        "                # Convert from normalized [x, y, w, h] to pixel [x, y, w, h]\n",
        "                boxes_pixel = []\n",
        "                for bbox in boxes:\n",
        "                    boxes_pixel.append([\n",
        "                        bbox[0] * new_w,  # x\n",
        "                        bbox[1] * new_h,  # y\n",
        "                        bbox[2] * new_w,  # width\n",
        "                        bbox[3] * new_h   # height\n",
        "                    ])\n",
        "                boxes = boxes_pixel\n",
        "            else:\n",
        "                # Use transform without bbox\n",
        "                transformed = self.transform_no_bbox(image=image)\n",
        "                image = transformed['image']\n",
        "        else:\n",
        "            # For validation, no bbox_params needed\n",
        "            transformed = self.transform(image=image)\n",
        "            image = transformed['image']\n",
        "        \n",
        "        # Convert to tensors\n",
        "        if len(boxes) > 0:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            # Convert COCO format [x, y, w, h] to x1,y1,x2,y2\n",
        "            boxes_xyxy = boxes.clone()\n",
        "            boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2]  # x2 = x + w\n",
        "            boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3]  # y2 = y + h\n",
        "            \n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "            \n",
        "            # Convert masks to tensors - handle both numpy arrays and tensors\n",
        "            mask_tensors = []\n",
        "            for m in masks:\n",
        "                if isinstance(m, torch.Tensor):\n",
        "                    mask_tensors.append(m)\n",
        "                elif isinstance(m, np.ndarray):\n",
        "                    mask_tensors.append(torch.from_numpy(m))\n",
        "                else:\n",
        "                    # Fallback: convert to numpy first\n",
        "                    mask_tensors.append(torch.from_numpy(np.array(m)))\n",
        "            masks = torch.stack(mask_tensors)\n",
        "        else:\n",
        "            boxes_xyxy = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            masks = torch.zeros((0, *self.image_size), dtype=torch.uint8)\n",
        "        \n",
        "        target = {\n",
        "            'boxes': boxes_xyxy,\n",
        "            'labels': labels,\n",
        "            'masks': masks,\n",
        "            'image_id': torch.tensor([img_id])\n",
        "        }\n",
        "        \n",
        "        return image, target\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate for DataLoader\"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "print(\"âœ“ Dataset class defined!\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5.2 Create Datasets and DataLoaders (moved here after class definition)\n",
        "# ============================================================================\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = WoundDataset(\n",
        "    '../data/splits/train.json',\n",
        "    CONFIG['data_root'],\n",
        "    tuple(CONFIG['image_size']),\n",
        "    is_train=True\n",
        ")\n",
        "\n",
        "val_dataset = WoundDataset(\n",
        "    '../data/splits/val.json',\n",
        "    CONFIG['data_root'],\n",
        "    tuple(CONFIG['image_size']),\n",
        "    is_train=False\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(\"âœ“ Datasets and DataLoaders created!\")\n",
        "print(f\"âœ“ Train samples: {len(train_dataset)}\")\n",
        "print(f\"âœ“ Val samples: {len(val_dataset)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5.4 Setup Optimizer and Scheduler\n",
        "# ============================================================================\n",
        "\n",
        "# Optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=CONFIG['learning_rate'], momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "print(\"âœ“ Optimizer and scheduler setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Part 6: Start Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Training Loop\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "checkpoints_dir = Path(CONFIG['checkpoints_dir'])\n",
        "checkpoints_dir.mkdir(exist_ok=True)\n",
        "\n",
        "for epoch in range(1, CONFIG['epochs'] + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{CONFIG['epochs']}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Train\n",
        "    train_loss = train_one_epoch(model, optimizer, train_loader, CONFIG['device'])\n",
        "    \n",
        "    # Validate\n",
        "    val_loss = validate(model, val_loader, CONFIG['device'])\n",
        "    \n",
        "    # Step scheduler\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    # Print stats\n",
        "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "        }, checkpoints_dir / 'best_model.pth')\n",
        "        print(f\"âœ“ Saved best model (val_loss: {val_loss:.4f})\")\n",
        "    \n",
        "    # Save checkpoint every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, checkpoints_dir / f'checkpoint_epoch_{epoch}.pth')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” Part 7: Prediction Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 7.1 Helper Functions for Prediction\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_wound_area(predictions, marker_class_id=8, marker_size_cm=3.0):\n",
        "    \"\"\"Ø­Ø³Ø§Ø¨ Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø¬Ø±Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù‚ÙŠØ§Ø³ 3Ã—3 Ø³Ù…\"\"\"\n",
        "    \n",
        "    labels = predictions['labels'].cpu().numpy()\n",
        "    masks = predictions['masks'].cpu().numpy()\n",
        "    \n",
        "    # Find marker\n",
        "    marker_idx = np.where(labels == marker_class_id)[0]\n",
        "    \n",
        "    if len(marker_idx) == 0:\n",
        "        return None, None\n",
        "    \n",
        "    # Get marker mask\n",
        "    marker_mask = masks[marker_idx[0]][0] > 0.5\n",
        "    marker_area_pixels = marker_mask.sum()\n",
        "    \n",
        "    if marker_area_pixels == 0:\n",
        "        return None, None\n",
        "    \n",
        "    # Calculate pixel to cm ratio\n",
        "    pixel_to_cm = marker_size_cm / np.sqrt(marker_area_pixels)\n",
        "    \n",
        "    # Find wound (class 0)\n",
        "    wound_idx = np.where(labels == 0)[0]\n",
        "    \n",
        "    if len(wound_idx) == 0:\n",
        "        return None, pixel_to_cm\n",
        "    \n",
        "    # Get wound mask\n",
        "    wound_mask = masks[wound_idx[0]][0] > 0.5\n",
        "    wound_area_pixels = wound_mask.sum()\n",
        "    \n",
        "    # Convert to cmÂ²\n",
        "    wound_area_cm2 = wound_area_pixels * (pixel_to_cm ** 2)\n",
        "    \n",
        "    return wound_area_cm2, pixel_to_cm\n",
        "\n",
        "\n",
        "def detect_infection(predictions, infection_classes=[4, 5, 6, 15]):\n",
        "    \"\"\"ÙƒØ´Ù Ø§Ù„Ø¹Ø¯ÙˆÙ‰ Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©\"\"\"\n",
        "    \n",
        "    labels = predictions['labels'].cpu().numpy()\n",
        "    scores = predictions['scores'].cpu().numpy()\n",
        "    \n",
        "    # Check for infection indicators\n",
        "    infection_detections = []\n",
        "    \n",
        "    for label, score in zip(labels, scores):\n",
        "        if label in infection_classes:\n",
        "            infection_detections.append(float(score))\n",
        "    \n",
        "    if len(infection_detections) > 0:\n",
        "        return True, np.mean(infection_detections)\n",
        "    \n",
        "    return False, 0.0\n",
        "\n",
        "print(\"âœ“ Helper functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 7.2 Prediction Function\n",
        "# ============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_image(image_path: str, model, device, conf_threshold=0.5):\n",
        "    \"\"\"Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© ÙˆØ§Ø­Ø¯Ø©\"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Convert to absolute path if needed\n",
        "    img_path = Path(image_path)\n",
        "    if not img_path.is_absolute():\n",
        "        current_dir = Path.cwd()\n",
        "        if current_dir.name == 'notebooks':\n",
        "            img_path = current_dir.parent / img_path\n",
        "        else:\n",
        "            img_path = current_dir / img_path\n",
        "    \n",
        "    # Load image\n",
        "    image = cv2.imread(str(img_path))\n",
        "    \n",
        "    # Check if image loaded successfully\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not load image from: {img_path}\\nPlease check the path and make sure the image exists.\")\n",
        "    \n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Resize\n",
        "    image_resized = cv2.resize(image_rgb, tuple(CONFIG['image_size']))\n",
        "    \n",
        "    # To tensor\n",
        "    image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float() / 255.0\n",
        "    \n",
        "    # Normalize\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    image_tensor = (image_tensor - mean) / std\n",
        "    \n",
        "    # Predict\n",
        "    predictions = model([image_tensor.to(device)])[0]\n",
        "    \n",
        "    # Filter by confidence\n",
        "    keep = predictions['scores'] >= conf_threshold\n",
        "    filtered = {\n",
        "        'boxes': predictions['boxes'][keep],\n",
        "        'labels': predictions['labels'][keep],\n",
        "        'scores': predictions['scores'][keep],\n",
        "        'masks': predictions['masks'][keep]\n",
        "    }\n",
        "    \n",
        "    # Calculate wound area\n",
        "    wound_area, _ = calculate_wound_area(filtered)\n",
        "    \n",
        "    # Detect infection\n",
        "    has_infection, infection_conf = detect_infection(filtered)\n",
        "    \n",
        "    # Build result\n",
        "    result = {\n",
        "        'image_path': image_path,\n",
        "        'num_detections': len(filtered['labels']),\n",
        "        'wound_area_cm2': float(wound_area) if wound_area else None,\n",
        "        'has_infection': has_infection,\n",
        "        'infection_confidence': float(infection_conf),\n",
        "        'findings': {\n",
        "            'edema': 4 in filtered['labels'].cpu().numpy(),\n",
        "            'hyperemia': 5 in filtered['labels'].cpu().numpy(),\n",
        "            'necrosis': 6 in filtered['labels'].cpu().numpy(),\n",
        "            'granulation': 7 in filtered['labels'].cpu().numpy(),\n",
        "            'fibrin': 1 in filtered['labels'].cpu().numpy(),\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return result, filtered\n",
        "\n",
        "print(\"âœ“ Prediction function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 7.3 Visualization Function\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_prediction(image_path: str, predictions):\n",
        "    \"\"\"Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©\"\"\"\n",
        "    \n",
        "    # Convert to absolute path if needed\n",
        "    img_path = Path(image_path)\n",
        "    if not img_path.is_absolute():\n",
        "        current_dir = Path.cwd()\n",
        "        if current_dir.name == 'notebooks':\n",
        "            img_path = current_dir.parent / img_path\n",
        "        else:\n",
        "            img_path = current_dir / img_path\n",
        "    \n",
        "    image = cv2.imread(str(img_path))\n",
        "    \n",
        "    # Check if image loaded successfully\n",
        "    if image is None:\n",
        "        print(f\"âš ï¸ Warning: Could not load image from: {img_path}\")\n",
        "        return\n",
        "    \n",
        "    image = cv2.resize(image, tuple(CONFIG['image_size']))\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    masks = predictions['masks'].cpu().numpy()\n",
        "    labels = predictions['labels'].cpu().numpy()\n",
        "    scores = predictions['scores'].cpu().numpy()\n",
        "    \n",
        "    # Draw masks\n",
        "    for mask, label, score in zip(masks, labels, scores):\n",
        "        mask = (mask[0] > 0.5).astype(np.uint8)\n",
        "        \n",
        "        # Random color\n",
        "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
        "        \n",
        "        # Apply mask\n",
        "        colored_mask = np.zeros_like(image_rgb)\n",
        "        colored_mask[mask > 0] = color\n",
        "        image_rgb = cv2.addWeighted(image_rgb, 0.7, colored_mask, 0.3, 0)\n",
        "    \n",
        "    # Display\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predictions: {len(labels)} detections\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"âœ“ Visualization function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Part 8: Run Prediction\n",
        "\n",
        "**Ø¹Ø¯Ù‘Ù„ Ù…Ø³Ø§Ø± Ø§Ù„ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø«Ù… Ø´ØºÙ‘Ù„Ù‡Ø§**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Load Model and Predict\n",
        "# ============================================================================\n",
        "\n",
        "# Load best model\n",
        "model_path = Path(CONFIG['checkpoints_dir']) / 'best_model.pth'\n",
        "\n",
        "if model_path.exists():\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    checkpoint = torch.load(model_path, map_location=CONFIG['device'])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    print(\"âœ“ Model loaded successfully!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Model not found! Please train the model first (Part 6)\")\n",
        "\n",
        "# ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø± Ø¥Ù„Ù‰ ØµÙˆØ±Ø© ØªØ±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„ÙŠÙ‡Ø§\n",
        "image_path = 'data/task_0/data/2.jpg'  # â¬…ï¸ Ø¹Ø¯Ù‘Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³Ø§Ø±\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Wound Detection - Prediction\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nImage: {image_path}\")\n",
        "\n",
        "# Predict\n",
        "print(\"\\nPredicting...\")\n",
        "result, predictions = predict_image(image_path, model, CONFIG['device'])\n",
        "\n",
        "# Print result\n",
        "print(\"\\nResults:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Detections: {result['num_detections']}\")\n",
        "print(f\"Wound Area: {result['wound_area_cm2']} cmÂ²\" if result['wound_area_cm2'] else \"Wound Area: N/A\")\n",
        "print(f\"Infection: {'YES' if result['has_infection'] else 'NO'} (confidence: {result['infection_confidence']:.2f})\")\n",
        "print(\"\\nFindings:\")\n",
        "for finding, present in result['findings'].items():\n",
        "    print(f\"  {finding}: {'âœ“' if present else 'âœ—'}\")\n",
        "\n",
        "# Visualize\n",
        "print(\"\\nVisualizing...\")\n",
        "visualize_prediction(image_path, predictions)\n",
        "\n",
        "# Save result\n",
        "output_dir = Path(CONFIG['results_dir'])\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "result_file = output_dir / f\"{Path(image_path).stem}_result.json\"\n",
        "with open(result_file, 'w') as f:\n",
        "    json.dump(result, f, indent=2)\n",
        "print(f\"\\nâœ“ Result saved: {result_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
