{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Wound Infection Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "‚úì Libraries imported!\n",
            "‚úì CONFIG loaded!\n",
            "‚úì Model built!\n",
            "‚úì Loading trained model from: E:\\GitHub\\Wound-infection-detection-model\\checkpoints_medical_aug\\best_model.pth\n",
            "‚úì Trained model loaded successfully!\n",
            "  - Epoch: 8\n",
            "  - Best Val Loss: 3.2684\n",
            "============================================================\n",
            "‚úÖ Quick Start Complete! You can now use the model for prediction.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Quick Start: Load Everything Automatically\n",
        "# ============================================================================\n",
        "# Run this cell first when opening a new notebook session!\n",
        "\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch Vision\n",
        "import torchvision\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úì Libraries imported!\")\n",
        "\n",
        "# ============================================================================\n",
        "# Load CONFIG\n",
        "# ============================================================================\n",
        "CONFIG = {\n",
        "    'num_classes': 17,  # Background + 16 classes\n",
        "    'batch_size': 2,\n",
        "    'num_workers': 2,\n",
        "    'learning_rate': 0.0005,\n",
        "    'num_epochs': 50,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'checkpoints_dir': '../checkpoints_medical_aug',\n",
        "}\n",
        "\n",
        "print(\"‚úì CONFIG loaded!\")\n",
        "\n",
        "# ============================================================================\n",
        "# Build Model\n",
        "# ============================================================================\n",
        "def build_model(num_classes=17):\n",
        "    \"\"\"ÿ®ŸÜÿßÿ° Mask R-CNN model\"\"\"\n",
        "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
        "    return model\n",
        "\n",
        "model = build_model(num_classes=CONFIG['num_classes'])\n",
        "model.to(CONFIG['device'])\n",
        "print(\"‚úì Model built!\")\n",
        "\n",
        "# ============================================================================\n",
        "# Load Trained Model (if exists)\n",
        "# ============================================================================\n",
        "# Find project root\n",
        "current_dir = Path.cwd()\n",
        "project_root = current_dir\n",
        "for parent in current_dir.parents:\n",
        "    if (parent / 'README.md').exists() or (parent / 'requirements.txt').exists():\n",
        "        project_root = parent\n",
        "        break\n",
        "project_root = Path(project_root).resolve()\n",
        "\n",
        "# Resolve checkpoint path\n",
        "checkpoints_dir = CONFIG['checkpoints_dir']\n",
        "if checkpoints_dir.startswith('../') or checkpoints_dir.startswith('..\\\\'):\n",
        "    checkpoints_dir = checkpoints_dir.replace('../', '').replace('..\\\\', '')\n",
        "checkpoints_dir = project_root / checkpoints_dir\n",
        "model_path = checkpoints_dir / 'best_model.pth'\n",
        "\n",
        "if model_path.exists():\n",
        "    print(f\"‚úì Loading trained model from: {model_path}\")\n",
        "    # weights_only=False is needed to load optimizer_state_dict, epoch, val_loss, etc.\n",
        "    checkpoint = torch.load(model_path, map_location=CONFIG['device'], weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    print(\"‚úì Trained model loaded successfully!\")\n",
        "    if 'epoch' in checkpoint:\n",
        "        print(f\"  - Epoch: {checkpoint['epoch']}\")\n",
        "    if 'val_loss' in checkpoint:\n",
        "        print(f\"  - Best Val Loss: {checkpoint['val_loss']:.4f}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  No trained model found at: {model_path}\")\n",
        "    print(\"   Run Part 6 (Training) to train a model first.\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ Quick Start Complete! You can now use the model for prediction.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Part 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "‚úì All libraries imported successfully!\n",
            "============================================================\n",
            "PyTorch: 2.5.1+cu121\n",
            "NumPy: 2.3.5\n",
            "CUDA: True\n",
            "Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Import all required libraries\n",
        "# ============================================================================\n",
        "\n",
        "try:\n",
        "    import json\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "    from pathlib import Path\n",
        "    from typing import Dict, List, Tuple\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    from tqdm import tqdm\n",
        "    import random\n",
        "    import yaml\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    # PyTorch Vision\n",
        "    import torchvision\n",
        "    from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "    from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"‚úì All libraries imported successfully!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"PyTorch: {torch.__version__}\")\n",
        "    print(f\"NumPy: {np.__version__}\")\n",
        "    print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "except ValueError as e:\n",
        "    if \"numpy.dtype size changed\" in str(e):\n",
        "        print(\"=\" * 60)\n",
        "        print(\"‚ùå ÿÆÿ∑ÿ£: ÿ™ÿπÿßÿ±ÿ∂ ÿ®ŸäŸÜ numpy Ÿà scipy\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\nüîß ÿßŸÑÿ≠ŸÑ:\")\n",
        "        print(\"   1. ÿ¥ÿ∫ŸëŸÑ Ÿáÿ∞Ÿá ÿßŸÑÿ£ŸàÿßŸÖÿ± ŸÅŸä Terminal:\")\n",
        "        print(\"      pip install --upgrade --force-reinstall numpy scipy\")\n",
        "        print(\"   2. ÿ£Ÿà ÿ¥ÿ∫ŸëŸÑ Part 0 ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ\")\n",
        "        print(\"   3. ÿ£ÿπÿØ ÿ™ÿ¥ÿ∫ŸäŸÑ Kernel: Kernel ‚Üí Restart\")\n",
        "        print(\"=\" * 60)\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "        \n",
        "except ImportError as e:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"‚ùå ÿÆÿ∑ÿ£ ŸÅŸä ÿßÿ≥ÿ™Ÿäÿ±ÿßÿØ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ÿßŸÑÿÆÿ∑ÿ£: {e}\")\n",
        "    print(\"\\nüîß ÿßŸÑÿ≠ŸÑ:\")\n",
        "    print(\"   1. ÿ¥ÿ∫ŸëŸÑ Part 0 ÿ£ŸàŸÑÿßŸã (ÿ™ÿ´ÿ®Ÿäÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™)\")\n",
        "    print(\"   2. ÿ£ÿπÿØ ÿ™ÿ¥ÿ∫ŸäŸÑ Kernel: Kernel ‚Üí Restart\")\n",
        "    print(\"=\" * 60)\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Part 2: Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üì¶ ÿ™ÿ´ÿ®Ÿäÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™...\n",
            "============================================================\n",
            "\n",
            "[1/3] ÿ™ÿ´ÿ®Ÿäÿ™ setuptools Ÿà wheel...\n",
            "  üì¶ setuptools... ‚úì\n",
            "  üì¶ wheel... ‚úì\n",
            "\n",
            "[2/3] ÿ™ÿ´ÿ®Ÿäÿ™ numpy Ÿà scipy...\n",
            "  üì¶ numpy>=1.26.0... ‚úì\n",
            "  üì¶ scipy>=1.11.0... ‚úì\n",
            "\n",
            "[3/3] ÿ™ÿ´ÿ®Ÿäÿ™ ÿ®ÿßŸÇŸä ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™...\n",
            "  üì¶ torch... ‚úì\n",
            "  üì¶ torchvision... ‚úì\n",
            "  üì¶ opencv-python... ‚úì\n",
            "  üì¶ Pillow... ‚úì\n",
            "  üì¶ albumentations... ‚úì\n",
            "  üì¶ pandas... ‚úì\n",
            "  üì¶ matplotlib... ‚úì\n",
            "  üì¶ seaborn... ‚úì\n",
            "  üì¶ tqdm... ‚úì\n",
            "  üì¶ scikit-learn... ‚úì\n",
            "  üì¶ pycocotools... ‚úì\n",
            "  üì¶ pyyaml... ‚úì\n",
            "  üì¶ jupyter... ‚úì\n",
            "  üì¶ ipywidgets... ‚úì\n",
            "\n",
            "============================================================\n",
            "‚úì ÿ™ŸÖ ÿßŸÑÿ™ÿ´ÿ®Ÿäÿ™!\n",
            "============================================================\n",
            "‚ö†Ô∏è ÿ£ÿπÿØ ÿ™ÿ¥ÿ∫ŸäŸÑ Kernel: Kernel ‚Üí Restart\n",
            "============================================================\n",
            "Configuration:\n",
            "  data_root: ../data\n",
            "  image_size: [1024, 1024]\n",
            "  num_classes: 17\n",
            "  batch_size: 2\n",
            "  num_workers: 0\n",
            "  epochs: 50\n",
            "  learning_rate: 0.0005\n",
            "  device: cuda\n",
            "  train_ratio: 0.7\n",
            "  val_ratio: 0.15\n",
            "  test_ratio: 0.15\n",
            "  checkpoints_dir: ../checkpoints_medical_aug\n",
            "  results_dir: ../results\n",
            "\n",
            "‚úì Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ÿ™ÿ´ÿ®Ÿäÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"ÿ™ÿ´ÿ®Ÿäÿ™ ŸÖŸÉÿ™ÿ®ÿ© Ÿàÿßÿ≠ÿØÿ© ŸÖÿπ ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑÿ£ÿÆÿ∑ÿßÿ°\"\"\"\n",
        "    try:\n",
        "        print(f\"  üì¶ {package}...\", end=\" \", flush=True)\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", package],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=False\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úì\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è (ŸÅÿ¥ŸÑ - ŸÇÿØ ÿ™ŸÉŸàŸÜ ŸÖŸàÿ¨ŸàÿØÿ© ÿ®ÿßŸÑŸÅÿπŸÑ)\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ÿÆÿ∑ÿ£: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üì¶ ÿ™ÿ´ÿ®Ÿäÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ÿ™ÿ´ÿ®Ÿäÿ™ setuptools Ÿà wheel ÿ£ŸàŸÑÿßŸã (ŸÖŸáŸÖ ŸÑŸÄ Python 3.13)\n",
        "print(\"\\n[1/3] ÿ™ÿ´ÿ®Ÿäÿ™ setuptools Ÿà wheel...\")\n",
        "install_package(\"setuptools\")\n",
        "install_package(\"wheel\")\n",
        "\n",
        "# ÿ™ÿ´ÿ®Ÿäÿ™ numpy Ÿà scipy (ÿ•ÿµÿØÿßÿ±ÿßÿ™ ÿ™ÿØÿπŸÖ Python 3.13)\n",
        "print(\"\\n[2/3] ÿ™ÿ´ÿ®Ÿäÿ™ numpy Ÿà scipy...\")\n",
        "install_package(\"numpy>=1.26.0\")\n",
        "install_package(\"scipy>=1.11.0\")\n",
        "\n",
        "# ÿ™ÿ´ÿ®Ÿäÿ™ ÿ®ÿßŸÇŸä ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™ (Ÿàÿßÿ≠ÿØÿ© ÿ™ŸÑŸà ÿßŸÑÿ£ÿÆÿ±Ÿâ)\n",
        "print(\"\\n[3/3] ÿ™ÿ´ÿ®Ÿäÿ™ ÿ®ÿßŸÇŸä ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™...\")\n",
        "packages = [\n",
        "    \"torch\", \"torchvision\",\n",
        "    \"opencv-python\", \"Pillow\", \"albumentations\",\n",
        "    \"pandas\", \"matplotlib\", \"seaborn\",\n",
        "    \"tqdm\", \"scikit-learn\", \"pycocotools\",\n",
        "    \"pyyaml\", \"jupyter\", \"ipywidgets\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    install_package(package)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì ÿ™ŸÖ ÿßŸÑÿ™ÿ´ÿ®Ÿäÿ™!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚ö†Ô∏è ÿ£ÿπÿØ ÿ™ÿ¥ÿ∫ŸäŸÑ Kernel: Kernel ‚Üí Restart\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration - ŸäŸÖŸÉŸÜŸÉ ÿ™ÿπÿØŸäŸÑ Ÿáÿ∞Ÿá ÿßŸÑŸÇŸäŸÖ\n",
        "# ============================================================================\n",
        "\n",
        "import platform\n",
        "\n",
        "CONFIG = {\n",
        "    # Data - ŸÖÿ≥ÿßÿ±ÿßÿ™ ŸÜÿ≥ÿ®Ÿäÿ© ŸÖŸÜ notebooks/ ÿ•ŸÑŸâ project root\n",
        "    'data_root': '../data',  # ŸÖÿ¨ŸÑÿØ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ£ÿµŸÑŸäÿ© (ŸÑŸÑÿ™ÿ≠ŸÇŸÇ validation)\n",
        "    # ŸÖŸÑÿßÿ≠ÿ∏ÿ©: ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑŸÖÿπÿ≤ÿ≤ÿ© ÿ™Ÿèÿ≥ÿ™ÿÆÿØŸÖ ŸÑŸÑÿ™ÿØÿ±Ÿäÿ® (../data/augmented/)\n",
        "    'image_size': [1024, 1024],\n",
        "    'num_classes': 17,  # 16 wound types + background\n",
        "    'batch_size': 2,\n",
        "    # ÿπŸÑŸâ Windowsÿå ÿßÿ≥ÿ™ÿÆÿØŸÖ num_workers=0 ŸÑÿ™ÿ¨ŸÜÿ® ŸÖÿ¥ÿßŸÉŸÑ multiprocessing\n",
        "    'num_workers': 0 if platform.system() == 'Windows' else 4,\n",
        "    \n",
        "    # Training\n",
        "    'epochs': 50,\n",
        "    'learning_rate': 0.0005,  # Reduced from 0.001 to prevent NaN loss\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \n",
        "    # Splits\n",
        "    'train_ratio': 0.7,\n",
        "    'val_ratio': 0.15,\n",
        "    'test_ratio': 0.15,\n",
        "    \n",
        "    # Paths - ŸÖÿ≥ÿßÿ±ÿßÿ™ ŸÜÿ≥ÿ®Ÿäÿ© ŸÖŸÜ notebooks/ ÿ•ŸÑŸâ project root\n",
        "    'checkpoints_dir': '../checkpoints_medical_aug',  # ŸÖÿ¨ŸÑÿØ checkpoints ŸÅŸä ÿßŸÑÿ¨ÿ∞ÿ±\n",
        "    'results_dir': '../results',  # ŸÖÿ¨ŸÑÿØ results ŸÅŸä ÿßŸÑÿ¨ÿ∞ÿ±\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"\\n‚úì Device: {CONFIG['device']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Part 3: Data Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Converter function defined!\n",
            "‚úì Splitter function defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 3.1 CVAT to COCO Converter\n",
        "# ============================================================================\n",
        "\n",
        "def convert_cvat_to_coco(data_root: str, output_file: str):\n",
        "    \"\"\"ÿ™ÿ≠ŸàŸäŸÑ ÿ™ÿπŸÑŸäŸÇÿßÿ™ CVAT ÿ•ŸÑŸâ ÿµŸäÿ∫ÿ© COCO\"\"\"\n",
        "    \n",
        "    # Convert to absolute path - handle relative paths correctly\n",
        "    current_dir = Path.cwd()\n",
        "    data_root = Path(data_root)\n",
        "    \n",
        "    # Handle relative paths - notebook is in notebooks/ directory\n",
        "    if not data_root.is_absolute():\n",
        "        # If path starts with ../, it's already relative to parent\n",
        "        if str(data_root).startswith('../'):\n",
        "            data_root = current_dir.parent / data_root\n",
        "        # If path is relative and we're in notebooks/, go up one level\n",
        "        elif current_dir.name == 'notebooks':\n",
        "            data_root = current_dir.parent / data_root\n",
        "        else:\n",
        "            data_root = current_dir / data_root\n",
        "    \n",
        "    data_root = data_root.resolve()\n",
        "    \n",
        "    # Check if project.json exists\n",
        "    project_file = data_root / \"project.json\"\n",
        "    if not project_file.exists():\n",
        "        print(\"=\" * 60)\n",
        "        print(\"‚ùå ÿÆÿ∑ÿ£: ŸÖŸÑŸÅ project.json ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑŸÖÿ∑ŸÑŸàÿ®: {project_file}\")\n",
        "        print(f\"ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑÿ≠ÿßŸÑŸä: {current_dir}\")\n",
        "        print(f\"data_root: {data_root}\")\n",
        "        print(\"\\nüîß ÿßŸÑÿ≠ŸÑ:\")\n",
        "        print(\"   1. ÿ™ÿ£ŸÉÿØ ÿ£ŸÜ ŸÖÿ¨ŸÑÿØ 'data' ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ÿßŸÑŸÖÿ¨ŸÑÿØ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä ŸÑŸÑŸÖÿ¥ÿ±Ÿàÿπ\")\n",
        "        print(\"   2. ÿ™ÿ£ŸÉÿØ ÿ£ŸÜ 'data/project.json' ŸÖŸàÿ¨ŸàÿØ\")\n",
        "        print(\"   3. ÿ£Ÿà ÿπÿØŸëŸÑ CONFIG['data_root'] ŸÅŸä Part 2\")\n",
        "        print(\"=\" * 60)\n",
        "        raise FileNotFoundError(f\"project.json not found at {project_file}\")\n",
        "    \n",
        "    # Load project info\n",
        "    with open(project_file, 'r', encoding='utf-8') as f:\n",
        "        project_info = json.load(f)\n",
        "    \n",
        "    # Create label mapping\n",
        "    label_map = {label['name']: idx for idx, label in enumerate(project_info['labels'])}\n",
        "    \n",
        "    # Initialize COCO structure\n",
        "    coco_data = {\n",
        "        'images': [],\n",
        "        'annotations': [],\n",
        "        'categories': [{'id': idx, 'name': name} for name, idx in label_map.items()]\n",
        "    }\n",
        "    \n",
        "    image_id = 0\n",
        "    annotation_id = 0\n",
        "    \n",
        "    # Process all tasks\n",
        "    task_folders = sorted([f for f in data_root.iterdir() if f.is_dir() and f.name.startswith('task_')])\n",
        "    \n",
        "    print(f\"Processing {len(task_folders)} tasks...\")\n",
        "    \n",
        "    for task_folder in tqdm(task_folders):\n",
        "        try:\n",
        "            # Load annotations\n",
        "            with open(task_folder / \"annotations.json\", 'r', encoding='utf-8') as f:\n",
        "                annotations = json.load(f)\n",
        "            \n",
        "            # Get images\n",
        "            data_dir = task_folder / \"data\"\n",
        "            image_files = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*.png'))\n",
        "            \n",
        "            for img_file in image_files:\n",
        "                # Read image to get size\n",
        "                img = cv2.imread(str(img_file))\n",
        "                if img is None:\n",
        "                    continue\n",
        "                \n",
        "                h, w = img.shape[:2]\n",
        "                \n",
        "                # Add image\n",
        "                is_infected = '-not-' not in img_file.name.lower()\n",
        "                coco_data['images'].append({\n",
        "                    'id': image_id,\n",
        "                    'file_name': str(img_file.relative_to(data_root)),\n",
        "                    'width': w,\n",
        "                    'height': h,\n",
        "                    'infection_status': is_infected\n",
        "                })\n",
        "                \n",
        "                # Add annotations for this image (first frame)\n",
        "                if len(annotations) > 0 and 'shapes' in annotations[0]:\n",
        "                    for shape in annotations[0]['shapes']:\n",
        "                        if shape['type'] != 'polygon' or shape['label'] not in label_map:\n",
        "                            continue\n",
        "                        \n",
        "                        # Convert polygon points - handle different formats\n",
        "                        points = shape['points']\n",
        "                        \n",
        "                        # Check if points is a list of lists or a flat list\n",
        "                        if not isinstance(points, list):\n",
        "                            continue\n",
        "                        \n",
        "                        # Handle case where points might be a flat list of floats\n",
        "                        if len(points) > 0 and isinstance(points[0], (int, float)):\n",
        "                            # Flat list: [x1, y1, x2, y2, ...]\n",
        "                            if len(points) % 2 != 0:\n",
        "                                continue\n",
        "                            points = [[points[i], points[i+1]] for i in range(0, len(points), 2)]\n",
        "                        \n",
        "                        # Ensure points is a list of [x, y] pairs\n",
        "                        if not all(isinstance(p, (list, tuple)) and len(p) == 2 for p in points):\n",
        "                            continue\n",
        "                        \n",
        "                        polygon = [coord for point in points for coord in point]\n",
        "                        \n",
        "                        # Calculate bbox\n",
        "                        x_coords = [p[0] for p in points]\n",
        "                        y_coords = [p[1] for p in points]\n",
        "                        x_min, x_max = min(x_coords), max(x_coords)\n",
        "                        y_min, y_max = min(y_coords), max(y_coords)\n",
        "                        bbox = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "                        \n",
        "                        # Calculate area\n",
        "                        area = (x_max - x_min) * (y_max - y_min)\n",
        "                        \n",
        "                        coco_data['annotations'].append({\n",
        "                            'id': annotation_id,\n",
        "                            'image_id': image_id,\n",
        "                            'category_id': label_map[shape['label']],\n",
        "                            'segmentation': [polygon],\n",
        "                            'area': area,\n",
        "                            'bbox': bbox,\n",
        "                            'iscrowd': 0\n",
        "                        })\n",
        "                        annotation_id += 1\n",
        "                \n",
        "                image_id += 1\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {task_folder.name}: {e}\")\n",
        "    \n",
        "    # Save COCO file\n",
        "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(coco_data, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n‚úì Done! Saved to {output_file}\")\n",
        "    print(f\"‚úì Total images: {len(coco_data['images'])}\")\n",
        "    print(f\"‚úì Total annotations: {len(coco_data['annotations'])}\")\n",
        "    \n",
        "    return coco_data\n",
        "\n",
        "print(\"‚úì Converter function defined!\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3.2 Dataset Splitter\n",
        "# ============================================================================\n",
        "\n",
        "def split_dataset(coco_file: str, output_dir: str, train_r=0.7, val_r=0.15, test_r=0.15):\n",
        "    \"\"\"ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ•ŸÑŸâ train/val/test\"\"\"\n",
        "    \n",
        "    with open(coco_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "    \n",
        "    images = coco_data['images']\n",
        "    random.shuffle(images)\n",
        "    \n",
        "    n = len(images)\n",
        "    n_train = int(n * train_r)\n",
        "    n_val = int(n * val_r)\n",
        "    \n",
        "    splits = {\n",
        "        'train': images[:n_train],\n",
        "        'val': images[n_train:n_train+n_val],\n",
        "        'test': images[n_train+n_val:]\n",
        "    }\n",
        "    \n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    for split_name, split_images in splits.items():\n",
        "        split_ids = {img['id'] for img in split_images}\n",
        "        split_anns = [ann for ann in coco_data['annotations'] if ann['image_id'] in split_ids]\n",
        "        \n",
        "        split_data = {\n",
        "            'images': split_images,\n",
        "            'annotations': split_anns,\n",
        "            'categories': coco_data['categories']\n",
        "        }\n",
        "        \n",
        "        output_file = Path(output_dir) / f'{split_name}.json'\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(split_data, f, indent=2)\n",
        "        \n",
        "        print(f\"‚úì {split_name}: {len(split_images)} images, {len(split_anns)} annotations\")\n",
        "\n",
        "print(\"‚úì Splitter function defined!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Part 5: Model Building \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model built with 17 classes\n",
            "‚úì Model moved to cuda\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5.1 Build Model\n",
        "# ============================================================================\n",
        "\n",
        "def build_model(num_classes=17):\n",
        "    \"\"\"ÿ®ŸÜÿßÿ° Mask R-CNN model\"\"\"\n",
        "    \n",
        "    # Load pretrained model\n",
        "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
        "    \n",
        "    # Replace box predictor\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    \n",
        "    # Replace mask predictor\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = build_model(num_classes=CONFIG['num_classes'])\n",
        "model.to(CONFIG['device'])\n",
        "print(f\"‚úì Model built with {CONFIG['num_classes']} classes\")\n",
        "print(f\"‚úì Model moved to {CONFIG['device']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Training functions defined!\n",
            "‚úì Dataset class defined!\n",
            "üîç DEBUG: data_root resolved to: E:\\GitHub\\Wound-infection-detection-model\\data\\augmented\n",
            "üîç DEBUG: data_root exists: True\n",
            "üîç DEBUG: Contents (first 3): ['annotations_augmented.json', 'images']\n",
            "‚úì Datasets and DataLoaders created!\n",
            "‚úì Train samples: 316\n",
            "‚úì Val samples: 16\n",
            "‚úì Optimizer and scheduler setup complete!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5.2 Create Datasets and DataLoaders\n",
        "# ============================================================================\n",
        "\n",
        "# Note: WoundDataset class is defined later in this cell\n",
        "# The dataset creation code is moved to the end of this cell, after the class definition\n",
        "# Make sure to run this entire cell from the beginning\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5.3 Training Functions\n",
        "# ============================================================================\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device):\n",
        "    \"\"\"ÿ™ÿØÿ±Ÿäÿ® epoch Ÿàÿßÿ≠ÿØ\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    valid_batches = 0\n",
        "    \n",
        "    pbar = tqdm(data_loader, desc=\"Training\")\n",
        "    \n",
        "    for images, targets in pbar:\n",
        "        # Move to device\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        # Forward\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        # Check for NaN/Inf loss\n",
        "        if torch.isnan(losses) or torch.isinf(losses):\n",
        "            print(f\"‚ö†Ô∏è Warning: NaN/Inf loss detected, skipping batch\")\n",
        "            continue\n",
        "        \n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        \n",
        "        # Gradient clipping to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += losses.item()\n",
        "        valid_batches += 1\n",
        "        pbar.set_postfix({'loss': f'{losses.item():.4f}'})\n",
        "    \n",
        "    return total_loss / valid_batches if valid_batches > 0 else float('inf')\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, data_loader, device):\n",
        "    \"\"\"Validation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    valid_batches = 0\n",
        "    \n",
        "    pbar = tqdm(data_loader, desc=\"Validation\")\n",
        "    \n",
        "    for images, targets in pbar:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        # Get loss\n",
        "        model.train()  # Need to be in train mode to get loss\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        model.eval()\n",
        "        \n",
        "        # Check for NaN/Inf loss\n",
        "        if torch.isnan(losses) or torch.isinf(losses):\n",
        "            print(f\"‚ö†Ô∏è Warning: NaN/Inf loss detected in validation, skipping batch\")\n",
        "            continue\n",
        "        \n",
        "        total_loss += losses.item()\n",
        "        valid_batches += 1\n",
        "        pbar.set_postfix({'loss': f'{losses.item():.4f}'})\n",
        "    \n",
        "    return total_loss / valid_batches if valid_batches > 0 else float('inf')\n",
        "\n",
        "print(\"‚úì Training functions defined!\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3.3 PyTorch Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class WoundDataset(Dataset):\n",
        "    \"\"\"Dataset ÿ®ÿ≥Ÿäÿ∑ ŸÑŸÑÿ¨ÿ±Ÿàÿ≠\"\"\"\n",
        "    \n",
        "    def __init__(self, coco_file: str, data_root: str, image_size=(1024, 1024), is_train=True):\n",
        "        # Find project root by looking for README.md or requirements.txt\n",
        "        # Start from current directory and go up until we find project root\n",
        "        current_dir = Path.cwd()\n",
        "        project_root = None\n",
        "        \n",
        "        # Strategy 1: Try to find project root by looking for README.md or requirements.txt\n",
        "        # Start from current_dir and go up\n",
        "        search_dir = current_dir\n",
        "        max_levels = 10  # Search up to 10 levels\n",
        "        for level in range(max_levels):\n",
        "            if (search_dir / 'README.md').exists() or (search_dir / 'requirements.txt').exists():\n",
        "                project_root = search_dir\n",
        "                break\n",
        "            # Check if we've reached filesystem root\n",
        "            parent = search_dir.parent\n",
        "            if parent == search_dir:  # Reached filesystem root\n",
        "                break\n",
        "            search_dir = parent\n",
        "        \n",
        "        # Strategy 2: If project root not found, try to find 'notebooks' directory\n",
        "        if project_root is None:\n",
        "            search_dir = current_dir\n",
        "            for _ in range(10):\n",
        "                # If we find a 'notebooks' directory, its parent is likely the project root\n",
        "                if (search_dir / 'notebooks').exists() and (search_dir / 'notebooks').is_dir():\n",
        "                    project_root = search_dir\n",
        "                    break\n",
        "                # Also check if current directory is 'notebooks'\n",
        "                if search_dir.name == 'notebooks':\n",
        "                    potential_root = search_dir.parent\n",
        "                    if (potential_root / 'README.md').exists() or (potential_root / 'requirements.txt').exists():\n",
        "                        project_root = potential_root\n",
        "                        break\n",
        "                parent = search_dir.parent\n",
        "                if parent == search_dir:\n",
        "                    break\n",
        "                search_dir = parent\n",
        "        \n",
        "        # Strategy 3: If still not found, try to find 'data' directory with 'splits' subdirectory\n",
        "        if project_root is None:\n",
        "            search_dir = current_dir\n",
        "            for _ in range(10):\n",
        "                if (search_dir / 'data' / 'splits').exists() and (search_dir / 'data' / 'splits').is_dir():\n",
        "                    project_root = search_dir\n",
        "                    break\n",
        "                parent = search_dir.parent\n",
        "                if parent == search_dir:\n",
        "                    break\n",
        "                search_dir = parent\n",
        "        \n",
        "        # Fallback: use current_dir\n",
        "        if project_root is None:\n",
        "            project_root = current_dir\n",
        "        \n",
        "        # Ensure project_root is absolute and resolved\n",
        "        project_root = Path(project_root).resolve()\n",
        "        \n",
        "        # Resolve data_root path\n",
        "        data_root = Path(data_root)\n",
        "        if not data_root.is_absolute():\n",
        "            # Handle relative paths (both '../' and '..\\\\' formats)\n",
        "            data_root_str = str(data_root)\n",
        "            # Normalize path separators and check for parent directory reference\n",
        "            if data_root_str.startswith('../') or data_root_str.startswith('..\\\\'):\n",
        "                # Remove '../' or '..\\\\' prefix and resolve from project root\n",
        "                # Remove first 3 characters ('../' or '..\\\\')\n",
        "                relative_path = data_root_str[3:]\n",
        "                # Normalize path separators\n",
        "                relative_path = relative_path.replace('\\\\', '/')\n",
        "                data_root = project_root / relative_path\n",
        "            else:\n",
        "                data_root = project_root / data_root\n",
        "        \n",
        "        self.data_root = data_root.resolve()\n",
        "        \n",
        "        # Debug: Print data_root for augmented data\n",
        "        if 'augmented' in str(self.data_root):\n",
        "            print(f\"üîç DEBUG: data_root resolved to: {self.data_root}\")\n",
        "            print(f\"üîç DEBUG: data_root exists: {self.data_root.exists()}\")\n",
        "            if self.data_root.exists():\n",
        "                contents = list(self.data_root.iterdir())\n",
        "                print(f\"üîç DEBUG: Contents (first 3): {[str(c.name) for c in contents[:3]]}\")\n",
        "        \n",
        "        # Handle coco_file path - resolve relative to project root\n",
        "        # project_root is already resolved to absolute path\n",
        "        coco_file_str = str(coco_file)\n",
        "        if not Path(coco_file).is_absolute():\n",
        "            if coco_file_str.startswith('../'):\n",
        "                # Remove '../' prefix and build path relative to project_root\n",
        "                relative_path = coco_file_str[3:]  # Remove '../'\n",
        "                # Build absolute path directly\n",
        "                coco_file_path = project_root / relative_path\n",
        "            else:\n",
        "                # Path is relative but doesn't start with ../\n",
        "                coco_file_path = project_root / coco_file_str\n",
        "        else:\n",
        "            # Path is already absolute\n",
        "            coco_file_path = Path(coco_file)\n",
        "        \n",
        "        # Resolve to absolute path (should already be absolute, but resolve() handles any remaining .. or .)\n",
        "        coco_file = str(coco_file_path.resolve())\n",
        "        \n",
        "        # Verify file exists before trying to open\n",
        "        if not Path(coco_file).exists():\n",
        "            # Additional debugging: check if the path components are correct\n",
        "            debug_info = (\n",
        "                f\"COCO annotation file not found: {coco_file}\\n\"\n",
        "                f\"Project root: {project_root}\\n\"\n",
        "                f\"Project root type: {type(project_root)}\\n\"\n",
        "                f\"Current dir: {current_dir}\\n\"\n",
        "                f\"Original coco_file parameter: {coco_file}\\n\"\n",
        "                f\"Relative path extracted: {relative_path if 'relative_path' in locals() else 'N/A'}\\n\"\n",
        "                f\"Path before resolve: {coco_file_path}\\n\"\n",
        "                f\"Path after resolve: {coco_file}\\n\"\n",
        "                f\"Please check the path and ensure the file exists.\"\n",
        "            )\n",
        "            raise FileNotFoundError(debug_info)\n",
        "        self.image_size = image_size\n",
        "        self.is_train = is_train\n",
        "        \n",
        "        # Load COCO data\n",
        "        with open(coco_file, 'r', encoding='utf-8') as f:\n",
        "            self.coco = json.load(f)\n",
        "        \n",
        "        # Filter out images that don't exist\n",
        "        valid_images = []\n",
        "        missing_count = 0\n",
        "        missing_samples = []  # Store first few missing paths for debugging\n",
        "        \n",
        "        for img_info in self.coco['images']:\n",
        "            img_path = self.data_root / img_info['file_name']\n",
        "            if img_path.exists():\n",
        "                valid_images.append(img_info)\n",
        "            else:\n",
        "                missing_count += 1\n",
        "                if len(missing_samples) < 3:  # Store first 3 missing paths\n",
        "                    missing_samples.append(str(img_path))\n",
        "        \n",
        "        self.images = valid_images\n",
        "        \n",
        "        if missing_count > 0:\n",
        "            print(f\"‚ö†Ô∏è Skipped {missing_count} missing images. Using {len(self.images)} valid images.\")\n",
        "            if len(self.images) == 0 and missing_samples:\n",
        "                print(f\"‚ö†Ô∏è ERROR: All images are missing! Sample missing paths:\")\n",
        "                for path in missing_samples:\n",
        "                    print(f\"   - {path}\")\n",
        "                print(f\"   data_root: {self.data_root}\")\n",
        "                print(f\"   data_root exists: {self.data_root.exists()}\")\n",
        "        \n",
        "        # Create annotation index (only for valid images)\n",
        "        valid_image_ids = {img['id'] for img in self.images}\n",
        "        self.img_to_anns = {}\n",
        "        for ann in self.coco['annotations']:\n",
        "            img_id = ann['image_id']\n",
        "            if img_id in valid_image_ids:  # Only index annotations for valid images\n",
        "                if img_id not in self.img_to_anns:\n",
        "                    self.img_to_anns[img_id] = []\n",
        "                self.img_to_anns[img_id].append(ann)\n",
        "        \n",
        "        # Setup transforms\n",
        "        if self.is_train:\n",
        "            # Transform with bbox support (for images with annotations)\n",
        "            self.transform_with_bbox = A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "                A.Resize(*self.image_size),\n",
        "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ToTensorV2()\n",
        "            ], bbox_params=A.BboxParams(format='coco', label_fields=['labels']))\n",
        "            # Transform without bbox (for images without annotations)\n",
        "            self.transform_no_bbox = A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "                A.Resize(*self.image_size),\n",
        "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "            self.transform = None  # Will use transform_with_bbox or transform_no_bbox in __getitem__\n",
        "        else:\n",
        "            # For validation, no bbox_params needed\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(*self.image_size),\n",
        "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "            self.transform_with_bbox = None\n",
        "            self.transform_no_bbox = None\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.images[idx]\n",
        "        img_id = img_info['id']\n",
        "        \n",
        "        # Load image\n",
        "        img_path = self.data_root / img_info['file_name']\n",
        "        image = cv2.imread(str(img_path))\n",
        "        \n",
        "        # Check if image loaded successfully\n",
        "        if image is None:\n",
        "            # This should not happen if filtering worked correctly, but handle gracefully\n",
        "            print(f\"‚ö†Ô∏è Warning: Could not load image: {img_path}\")\n",
        "            # Skip by returning next valid image (fallback)\n",
        "            return self.__getitem__((idx + 1) % len(self.images))\n",
        "        \n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Get annotations\n",
        "        anns = self.img_to_anns.get(img_id, [])\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "        \n",
        "        # Get image dimensions for normalization (use actual image dimensions, not JSON)\n",
        "        img_h, img_w = image.shape[:2]\n",
        "        \n",
        "        for ann in anns:\n",
        "            # Create mask from polygon first to validate segmentation\n",
        "            # Use actual image dimensions, not JSON dimensions (they may differ)\n",
        "            mask = np.zeros((img_h, img_w), dtype=np.uint8)\n",
        "            valid_segmentation = False\n",
        "            \n",
        "            # Check if segmentation exists and is valid\n",
        "            if 'segmentation' in ann and ann['segmentation']:\n",
        "                # Calculate scale factors if image dimensions differ from JSON\n",
        "                json_h, json_w = img_info.get('height', img_h), img_info.get('width', img_w)\n",
        "                scale_x = img_w / json_w if json_w > 0 else 1.0\n",
        "                scale_y = img_h / json_h if json_h > 0 else 1.0\n",
        "                \n",
        "                for seg in ann['segmentation']:\n",
        "                    if not isinstance(seg, list) or len(seg) < 6:  # Need at least 3 points (x,y pairs)\n",
        "                        continue\n",
        "                    try:\n",
        "                        seg_array = np.array(seg)\n",
        "                        if seg_array.size < 6:  # Need at least 3 points\n",
        "                            continue\n",
        "                        # Reshape to (N, 2) where N is number of points\n",
        "                        if seg_array.size % 2 != 0:\n",
        "                            continue  # Must be even number for (x,y) pairs\n",
        "                        poly = seg_array.reshape(-1, 2).astype(np.float32)\n",
        "                        if len(poly) < 3:  # Need at least 3 points for a polygon\n",
        "                            continue\n",
        "                        # Scale coordinates if image dimensions differ from JSON\n",
        "                        if scale_x != 1.0 or scale_y != 1.0:\n",
        "                            poly[:, 0] *= scale_x  # Scale x coordinates\n",
        "                            poly[:, 1] *= scale_y  # Scale y coordinates\n",
        "                        poly = poly.astype(np.int32)\n",
        "                        # Clip to image bounds\n",
        "                        poly[:, 0] = np.clip(poly[:, 0], 0, img_w - 1)\n",
        "                        poly[:, 1] = np.clip(poly[:, 1], 0, img_h - 1)\n",
        "                        cv2.fillPoly(mask, [poly], 1)\n",
        "                        valid_segmentation = True\n",
        "                    except (ValueError, TypeError) as e:\n",
        "                        # Skip invalid segmentation\n",
        "                        continue\n",
        "            \n",
        "            # Only add annotation if we have valid segmentation\n",
        "            if not valid_segmentation:\n",
        "                continue  # Skip this annotation entirely\n",
        "            \n",
        "            # COCO bbox format: [x, y, width, height] in pixels\n",
        "            bbox = ann['bbox']\n",
        "            # Normalize to [0, 1] range for albumentations\n",
        "            normalized_bbox = [\n",
        "                bbox[0] / img_w,  # x\n",
        "                bbox[1] / img_h,  # y\n",
        "                bbox[2] / img_w,  # width\n",
        "                bbox[3] / img_h   # height\n",
        "            ]\n",
        "            boxes.append(normalized_bbox)\n",
        "            labels.append(ann['category_id'])\n",
        "            masks.append(mask)\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.is_train:\n",
        "            if len(boxes) > 0:\n",
        "                # Use transform with bbox support\n",
        "                transformed = self.transform_with_bbox(\n",
        "                    image=image,\n",
        "                    bboxes=boxes,\n",
        "                    labels=labels,\n",
        "                    masks=masks\n",
        "                )\n",
        "                image = transformed['image']\n",
        "                boxes = transformed.get('bboxes', [])\n",
        "                labels = transformed.get('labels', [])\n",
        "                masks = transformed.get('masks', [])\n",
        "                \n",
        "                # Convert normalized bboxes back to pixel coordinates\n",
        "                # Get transformed image size\n",
        "                if isinstance(image, torch.Tensor):\n",
        "                    _, new_h, new_w = image.shape\n",
        "                else:\n",
        "                    new_h, new_w = image.shape[:2]\n",
        "                \n",
        "                # Convert from normalized [x, y, w, h] to pixel [x, y, w, h]\n",
        "                boxes_pixel = []\n",
        "                for bbox in boxes:\n",
        "                    boxes_pixel.append([\n",
        "                        bbox[0] * new_w,  # x\n",
        "                        bbox[1] * new_h,  # y\n",
        "                        bbox[2] * new_w,  # width\n",
        "                        bbox[3] * new_h   # height\n",
        "                    ])\n",
        "                boxes = boxes_pixel\n",
        "            else:\n",
        "                # Use transform without bbox\n",
        "                transformed = self.transform_no_bbox(image=image)\n",
        "                image = transformed['image']\n",
        "        else:\n",
        "            # For validation, no bbox_params needed\n",
        "            transformed = self.transform(image=image)\n",
        "            image = transformed['image']\n",
        "        \n",
        "        # Convert to tensors\n",
        "        if len(boxes) > 0:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            # Convert COCO format [x, y, w, h] to x1,y1,x2,y2\n",
        "            boxes_xyxy = boxes.clone()\n",
        "            boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2]  # x2 = x + w\n",
        "            boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3]  # y2 = y + h\n",
        "            \n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "            \n",
        "            # Convert masks to tensors - handle both numpy arrays and tensors\n",
        "            mask_tensors = []\n",
        "            for m in masks:\n",
        "                if isinstance(m, torch.Tensor):\n",
        "                    mask_tensors.append(m)\n",
        "                elif isinstance(m, np.ndarray):\n",
        "                    mask_tensors.append(torch.from_numpy(m))\n",
        "                else:\n",
        "                    # Fallback: convert to numpy first\n",
        "                    mask_tensors.append(torch.from_numpy(np.array(m)))\n",
        "            masks = torch.stack(mask_tensors)\n",
        "        else:\n",
        "            boxes_xyxy = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            masks = torch.zeros((0, *self.image_size), dtype=torch.uint8)\n",
        "        \n",
        "        target = {\n",
        "            'boxes': boxes_xyxy,\n",
        "            'labels': labels,\n",
        "            'masks': masks,\n",
        "            'image_id': torch.tensor([img_id])\n",
        "        }\n",
        "        \n",
        "        return image, target\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate for DataLoader\"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "print(\"‚úì Dataset class defined!\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5.2 Create Datasets and DataLoaders (moved here after class definition)\n",
        "# ============================================================================\n",
        "\n",
        "# Create datasets\n",
        "# Use augmented data for training (more data = better model)\n",
        "train_dataset = WoundDataset(\n",
        "    '../data/augmented/annotations_augmented.json',  # ‚úÖ Augmented data\n",
        "    '../data/augmented',  # ‚úÖ Path to augmented images\n",
        "    tuple(CONFIG['image_size']),\n",
        "    is_train=True\n",
        ")\n",
        "\n",
        "# Use original data for validation (real data, not augmented)\n",
        "val_dataset = WoundDataset(\n",
        "    '../data/splits/val.json',  # ‚úÖ Original validation data\n",
        "    CONFIG['data_root'],  # '../data' - original data path\n",
        "    tuple(CONFIG['image_size']),\n",
        "    is_train=False\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'],\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(\"‚úì Datasets and DataLoaders created!\")\n",
        "print(f\"‚úì Train samples: {len(train_dataset)}\")\n",
        "print(f\"‚úì Val samples: {len(val_dataset)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5.4 Setup Optimizer and Scheduler\n",
        "# ============================================================================\n",
        "\n",
        "# Optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=CONFIG['learning_rate'], momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "print(\"‚úì Optimizer and scheduler setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Part 6: Start Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Starting Training\n",
            "============================================================\n",
            "\n",
            "Epoch 1/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 143/158 [01:09<00:07,  2.04it/s, loss=0.0185]    \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     21\u001b[39m val_loss = validate(model, val_loader, CONFIG[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, data_loader, device)\u001b[39m\n\u001b[32m     26\u001b[39m targets = [{k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t.items()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m loss_dict = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m losses = \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict.values())\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Check for NaN/Inf loss\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch.Tensor):\n\u001b[32m    103\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m proposals, proposal_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m detections, detector_losses = \u001b[38;5;28mself\u001b[39m.roi_heads(features, proposals, images.image_sizes, targets)\n\u001b[32m    106\u001b[39m detections = \u001b[38;5;28mself\u001b[39m.transform.postprocess(detections, images.image_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:371\u001b[39m, in \u001b[36mRegionProposalNetwork.forward\u001b[39m\u001b[34m(self, images, features, targets)\u001b[39m\n\u001b[32m    367\u001b[39m objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m proposals = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbox_coder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m proposals = proposals.view(num_images, -\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m    373\u001b[39m boxes, scores = \u001b[38;5;28mself\u001b[39m.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torchvision\\models\\detection\\_utils.py:178\u001b[39m, in \u001b[36mBoxCoder.decode\u001b[39m\u001b[34m(self, rel_codes, boxes)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m box_sum > \u001b[32m0\u001b[39m:\n\u001b[32m    177\u001b[39m     rel_codes = rel_codes.reshape(box_sum, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m pred_boxes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m box_sum > \u001b[32m0\u001b[39m:\n\u001b[32m    180\u001b[39m     pred_boxes = pred_boxes.reshape(box_sum, -\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torchvision\\models\\detection\\_utils.py:216\u001b[39m, in \u001b[36mBoxCoder.decode_single\u001b[39m\u001b[34m(self, rel_codes, boxes)\u001b[39m\n\u001b[32m    213\u001b[39m pred_h = torch.exp(dh) * heights[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Distance from center to box's corner.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m c_to_c_h = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_ctr_y\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpred_h\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m * pred_h\n\u001b[32m    217\u001b[39m c_to_c_w = torch.tensor(\u001b[32m0.5\u001b[39m, dtype=pred_ctr_x.dtype, device=pred_w.device) * pred_w\n\u001b[32m    219\u001b[39m pred_boxes1 = pred_ctr_x - c_to_c_w\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Training Loop\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "checkpoints_dir = Path(CONFIG['checkpoints_dir'])\n",
        "checkpoints_dir.mkdir(exist_ok=True)\n",
        "\n",
        "for epoch in range(1, CONFIG['epochs'] + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{CONFIG['epochs']}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Train\n",
        "    train_loss = train_one_epoch(model, optimizer, train_loader, CONFIG['device'])\n",
        "    \n",
        "    # Validate\n",
        "    val_loss = validate(model, val_loader, CONFIG['device'])\n",
        "    \n",
        "    # Step scheduler\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    # Print stats\n",
        "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "        }, checkpoints_dir / 'best_model.pth')\n",
        "        print(f\"‚úì Saved best model (val_loss: {val_loss:.4f})\")\n",
        "    \n",
        "    # Save checkpoint every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, checkpoints_dir / f'checkpoint_epoch_{epoch}.pth')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Part 7: Prediction Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Helper functions defined!\n",
            "‚úì Prediction function defined!\n",
            "‚úì Visualization function defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.1 Helper Functions for Prediction\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_wound_area(predictions, marker_class_id=8, marker_size_cm=3.0):\n",
        "    \"\"\"ÿ≠ÿ≥ÿßÿ® ŸÖÿ≥ÿßÿ≠ÿ© ÿßŸÑÿ¨ÿ±ÿ≠ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿπŸÑÿßŸÖÿ© ÿßŸÑŸÇŸäÿßÿ≥ 3√ó3 ÿ≥ŸÖ\"\"\"\n",
        "    \n",
        "    labels = predictions['labels'].cpu().numpy()\n",
        "    masks = predictions['masks'].cpu().numpy()\n",
        "    \n",
        "    # Find marker\n",
        "    marker_idx = np.where(labels == marker_class_id)[0]\n",
        "    \n",
        "    if len(marker_idx) == 0:\n",
        "        return None, None\n",
        "    \n",
        "    # Get marker mask\n",
        "    marker_mask = masks[marker_idx[0]][0] > 0.5\n",
        "    marker_area_pixels = marker_mask.sum()\n",
        "    \n",
        "    if marker_area_pixels == 0:\n",
        "        return None, None\n",
        "    \n",
        "    # Calculate pixel to cm ratio\n",
        "    pixel_to_cm = marker_size_cm / np.sqrt(marker_area_pixels)\n",
        "    \n",
        "    # Find wound (class 0)\n",
        "    wound_idx = np.where(labels == 0)[0]\n",
        "    \n",
        "    if len(wound_idx) == 0:\n",
        "        return None, pixel_to_cm\n",
        "    \n",
        "    # Get wound mask\n",
        "    wound_mask = masks[wound_idx[0]][0] > 0.5\n",
        "    wound_area_pixels = wound_mask.sum()\n",
        "    \n",
        "    # Convert to cm¬≤\n",
        "    wound_area_cm2 = wound_area_pixels * (pixel_to_cm ** 2)\n",
        "    \n",
        "    return wound_area_cm2, pixel_to_cm\n",
        "\n",
        "\n",
        "def detect_infection(predictions, infection_classes=[4, 5, 6, 15]):\n",
        "    \"\"\"ŸÉÿ¥ŸÅ ÿßŸÑÿπÿØŸàŸâ ŸÖŸÜ ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ©\"\"\"\n",
        "    \n",
        "    labels = predictions['labels'].cpu().numpy()\n",
        "    scores = predictions['scores'].cpu().numpy()\n",
        "    \n",
        "    # Check for infection indicators\n",
        "    infection_detections = []\n",
        "    \n",
        "    for label, score in zip(labels, scores):\n",
        "        if label in infection_classes:\n",
        "            infection_detections.append(float(score))\n",
        "    \n",
        "    if len(infection_detections) > 0:\n",
        "        return True, np.mean(infection_detections)\n",
        "    \n",
        "    return False, 0.0\n",
        "\n",
        "print(\"‚úì Helper functions defined!\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7.2 Prediction Function\n",
        "# ============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_image(image_path: str, model, device, conf_threshold=0.5):\n",
        "    \"\"\"ÿßŸÑÿ™ŸÜÿ®ÿ§ ÿπŸÑŸâ ÿµŸàÿ±ÿ© Ÿàÿßÿ≠ÿØÿ©\"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Convert to absolute path if needed\n",
        "    img_path = Path(image_path)\n",
        "    if not img_path.is_absolute():\n",
        "        current_dir = Path.cwd()\n",
        "        if current_dir.name == 'notebooks':\n",
        "            img_path = current_dir.parent / img_path\n",
        "        else:\n",
        "            img_path = current_dir / img_path\n",
        "    \n",
        "    # Load image\n",
        "    image = cv2.imread(str(img_path))\n",
        "    \n",
        "    # Check if image loaded successfully\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not load image from: {img_path}\\nPlease check the path and make sure the image exists.\")\n",
        "    \n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Resize\n",
        "    image_resized = cv2.resize(image_rgb, tuple(CONFIG['image_size']))\n",
        "    \n",
        "    # To tensor\n",
        "    image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float() / 255.0\n",
        "    \n",
        "    # Normalize\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    image_tensor = (image_tensor - mean) / std\n",
        "    \n",
        "    # Predict\n",
        "    predictions = model([image_tensor.to(device)])[0]\n",
        "    \n",
        "    # Filter by confidence\n",
        "    keep = predictions['scores'] >= conf_threshold\n",
        "    filtered = {\n",
        "        'boxes': predictions['boxes'][keep],\n",
        "        'labels': predictions['labels'][keep],\n",
        "        'scores': predictions['scores'][keep],\n",
        "        'masks': predictions['masks'][keep]\n",
        "    }\n",
        "    \n",
        "    # Calculate wound area\n",
        "    wound_area, _ = calculate_wound_area(filtered)\n",
        "    \n",
        "    # Detect infection\n",
        "    has_infection, infection_conf = detect_infection(filtered)\n",
        "    \n",
        "    # Build result\n",
        "    result = {\n",
        "        'image_path': image_path,\n",
        "        'num_detections': len(filtered['labels']),\n",
        "        'wound_area_cm2': float(wound_area) if wound_area else None,\n",
        "        'has_infection': has_infection,\n",
        "        'infection_confidence': float(infection_conf),\n",
        "        'findings': {\n",
        "            'edema': 4 in filtered['labels'].cpu().numpy(),\n",
        "            'hyperemia': 5 in filtered['labels'].cpu().numpy(),\n",
        "            'necrosis': 6 in filtered['labels'].cpu().numpy(),\n",
        "            'granulation': 7 in filtered['labels'].cpu().numpy(),\n",
        "            'fibrin': 1 in filtered['labels'].cpu().numpy(),\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return result, filtered\n",
        "\n",
        "print(\"‚úì Prediction function defined!\")\n",
        "\n",
        "# ============================================================================\n",
        "# 7.3 Visualization Function\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_prediction(image_path: str, predictions):\n",
        "    \"\"\"ÿ±ÿ≥ŸÖ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ÿπŸÑŸâ ÿßŸÑÿµŸàÿ±ÿ©\"\"\"\n",
        "    \n",
        "    # Convert to absolute path if needed\n",
        "    img_path = Path(image_path)\n",
        "    if not img_path.is_absolute():\n",
        "        current_dir = Path.cwd()\n",
        "        if current_dir.name == 'notebooks':\n",
        "            img_path = current_dir.parent / img_path\n",
        "        else:\n",
        "            img_path = current_dir / img_path\n",
        "    \n",
        "    image = cv2.imread(str(img_path))\n",
        "    \n",
        "    # Check if image loaded successfully\n",
        "    if image is None:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not load image from: {img_path}\")\n",
        "        return\n",
        "    \n",
        "    image = cv2.resize(image, tuple(CONFIG['image_size']))\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    masks = predictions['masks'].cpu().numpy()\n",
        "    labels = predictions['labels'].cpu().numpy()\n",
        "    scores = predictions['scores'].cpu().numpy()\n",
        "    \n",
        "    # Draw masks\n",
        "    for mask, label, score in zip(masks, labels, scores):\n",
        "        mask = (mask[0] > 0.5).astype(np.uint8)\n",
        "        \n",
        "        # Random color\n",
        "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
        "        \n",
        "        # Apply mask\n",
        "        colored_mask = np.zeros_like(image_rgb)\n",
        "        colored_mask[mask > 0] = color\n",
        "        image_rgb = cv2.addWeighted(image_rgb, 0.7, colored_mask, 0.3, 0)\n",
        "    \n",
        "    # Display\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(image_rgb)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predictions: {len(labels)} detections\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Visualization function defined!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Part 8: Run Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: ..\\checkpoints_medical_aug\\best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ul8ziz\\AppData\\Local\\Temp\\ipykernel_29884\\2936441136.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path, map_location=CONFIG['device'])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model loaded successfully!\n",
            "\n",
            "============================================================\n",
            "Wound Detection - Prediction\n",
            "============================================================\n",
            "\n",
            "Image: data/task_0/data/2.jpg\n",
            "\n",
            "Predicting...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Could not load image from: e:\\GitHub\\Wound-infection-detection-model\\data\\task_0\\data\\2.jpg\nPlease check the path and make sure the image exists.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPredicting...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m result, predictions = \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Print result\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResults:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\GitHub\\Wound-infection-detection-model\\.venv_cuda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mpredict_image\u001b[39m\u001b[34m(image_path, model, device, conf_threshold)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Check if image loaded successfully\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not load image from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease check the path and make sure the image exists.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Resize\u001b[39;00m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Could not load image from: e:\\GitHub\\Wound-infection-detection-model\\data\\task_0\\data\\2.jpg\nPlease check the path and make sure the image exists."
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Load Model and Predict\n",
        "# ============================================================================\n",
        "\n",
        "# Load best model\n",
        "model_path = Path(CONFIG['checkpoints_dir']) / 'best_model.pth'\n",
        "\n",
        "if model_path.exists():\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    checkpoint = torch.load(model_path, map_location=CONFIG['device'])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    print(\"‚úì Model loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Model not found! Please train the model first (Part 6)\")\n",
        "\n",
        "# ÿ™ÿ∫ŸäŸäÿ± ÿßŸÑŸÖÿ≥ÿßÿ± ÿ•ŸÑŸâ ÿµŸàÿ±ÿ© ÿ™ÿ±ŸäÿØ ÿßŸÑÿ™ŸÜÿ®ÿ§ ÿπŸÑŸäŸáÿß\n",
        "image_path = 'data/task_0/data/task_0_image_001.jpg'  # ‚¨ÖÔ∏è ÿπÿØŸëŸÑ Ÿáÿ∞ÿß ÿßŸÑŸÖÿ≥ÿßÿ±\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Wound Detection - Prediction\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nImage: {image_path}\")\n",
        "\n",
        "# Predict\n",
        "print(\"\\nPredicting...\")\n",
        "result, predictions = predict_image(image_path, model, CONFIG['device'])\n",
        "\n",
        "# Print result\n",
        "print(\"\\nResults:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Detections: {result['num_detections']}\")\n",
        "print(f\"Wound Area: {result['wound_area_cm2']} cm¬≤\" if result['wound_area_cm2'] else \"Wound Area: N/A\")\n",
        "print(f\"Infection: {'YES' if result['has_infection'] else 'NO'} (confidence: {result['infection_confidence']:.2f})\")\n",
        "print(\"\\nFindings:\")\n",
        "for finding, present in result['findings'].items():\n",
        "    print(f\"  {finding}: {'‚úì' if present else '‚úó'}\")\n",
        "\n",
        "# Visualize\n",
        "print(\"\\nVisualizing...\")\n",
        "visualize_prediction(image_path, predictions)\n",
        "\n",
        "# Save result\n",
        "output_dir = Path(CONFIG['results_dir'])\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "result_file = output_dir / f\"{Path(image_path).stem}_result.json\"\n",
        "with open(result_file, 'w') as f:\n",
        "    json.dump(result, f, indent=2)\n",
        "print(f\"\\n‚úì Result saved: {result_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_cuda (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
